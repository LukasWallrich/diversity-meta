---
title: "Diversity and team-performance meta-analysis"
description: null
output:
  html_document:
    theme: united
    toc: yes
  html_notebook: default
  pdf_document:
    fig_height: 6
    fig_width: 8
  word_document: default
Author: Lukas Wallrich 
---

```{r setup, include=FALSE}
if (!require(groundhog)) install.packages('groundhog')
groundhog::groundhog.library(c("readxl", "TOSTER", "metafor", "tidyverse", "clubSandwich", "cli", "rsprite2", "esc",
                               "mice", "metacart", "gt", "gtExtras", "psych"), date = "2023-07-09")

# Need more recent version of patchwork due to faceting bug
groundhog::groundhog.library(c("sf", "rworldmap", "numform", "patchwork"), date = "2023-07-09")

groundhog::groundhog.library(c("lukaswallrich/timesaveR"), date = "2023-07-09")

source("helpers/helpers.R")

```

*Credits:* This code builds on the [RMarkdown template for Correlational studies meta-analysis in Psychology](https://osf.io/f85uy/), developed by Adrien Fillon and Gilad Feldman. 

```{r intro, include=FALSE}
# Read M/SD as character to retain trailing 0s for GRIM/GRIMMER
col_types_en <- rep("guess", 56)
col_types_en[c(33:35, 42:44)] <- "text"


effect_sizes_en <- read_excel("data/english_coding-WIP.xlsx", 
                              range = cell_limits(c(3, 1), c(NA, 56),
                                                  sheet = "Unified Coding"),
                              na = c("#N/A", "NA"),
                              col_types = col_types_en) %>% filter(!is.na(ID))

col_types_nen <- rep("guess", 59)
col_types_nen[c(36:38, 45:47)] <- "text"

effect_sizes_n_en <- read_excel("data/non-English_coding-WIP.xlsx",
                                range = cell_limits(c(3, 1), c(NA, 59),
                                                  sheet = "Unified Coding"),
                                na = c("#N/A", "NA"),
                                col_types = col_types_nen) %>% filter(!is.na(ID)) %>%
  filter(is.na(Excluded) | Excluded != "yes")


#effect_sizes_other <- read_excel("data/english_coding-WIP.xlsx")

# Assumed correlation between dependent effect sizes
rho <- 0.6
```



```{r importfile, include=FALSE}
# Rename variables
rename_vec_en <- c(
  NULL = "double coded",
  id = "ID",
  effect_id = "effect_id",
  #excl = "Excluded",
  NULL = "Title",
  author_year = "Author (Year)",
  NULL = "File", # Only contains "PDF", not the link
  file = "File URL",
  year = "Year",
  NULL = "Coder",
  NULL = "Status",
  NULL = "Date coded",
  study = "Study",
  sample = "Sample",
  art_focus = "Article focus",
  pub_status = "Publication status",
  nonlin_rel = "Non-linear relationship",
  gen_notes =  "General Notes",
  design = "Design",
  setting = "Setting",
  ind_sector = "Industry/sector",
  team_function = "Function",
  country = "Country",
  n_teams = "N teams",
  n_obs = "N obs",
  stud_sample = "Student sample",
  tmt = "TMT",
  year_coll = "Year collected (if reported)",
  domain = "Domain",
  sub_dom = "Sub-domain",
  div_specific = "Specify",
  meas_type = "Measure type",
  items_div = "Items...31",
  opts_div = "Options...32",
  m_div = "M...33",
  sd_div = "SD...34",
  reliab_div = "Reliability Cronbach's alpha (or note)...35",
  notes_div = "Notes...36",
  name_perf = "Name",
  criterion = "Criterion",
  rater = "Rater",
  items_perf = "Items...40",
  opts_perf = "Options...41",
  m_perf = "M...42",
  sd_perf = "SD...43",
  reliab_perf = "Reliability Cronbach's alpha (or note)...44",
  notes_perf = "Notes...45",
  r = "r",
  d = "d",
  other = "other",
  stats_notes = "Statistics Notes",
  interdep = "Interdependence",
  complexity = "Complexity",
  longevity = "Longevity",
  virtuality = "Virtuality",
  auth_diff = "Authority differentiation",
  div_climate = "Diversity climate",
  psych_safe = "Psych safety"
)

rename_vec_n_en <- c(
  id = "ID",
  effect_id = "rowid",
  NULL = "Title",
  NULL = "File Or", 
  NULL = "File En",
  file = "File Or URL", # Contains original URL
  year = "Year",
    author_year = "Author (Year)",
  NULL = "Coder",
  NULL = "Status",
  NULL = "Date coded",
  study = "Study",
  sample = "Sample",
  language = "Language",
  art_focus = "Article focus",
  pub_status = "Publication status",
  nonlin_rel = "Non-linear relationship",
  gen_notes = "General Notes",
  design = "Design",
  setting = "Setting",
  ind_sector = "Industry/sector", 
  team_function = "Function",
  country = "Country",
  n_teams = "N teams",
  n_obs = "N obs", 
  stud_sample = "Student sample",
  tmt = "TMT",
  year_coll = "Year collected (if reported)",
  domain = "Domain",
  sub_dom = "Sub-domain",
  div_dom_specific = "Specify...31",  
  meas_type = "Measure type",
  div_specific = "Specify...33", 
  items_div = "Items...34",  
  options_div = "Options...35",  
  m_div = "M...36",  
  sd_div = "SD...37", 
  reliab_div = "Reliability Cronbach's alpha (or note)...38",  
  notes_div = "Notes...39",  
  name_perf = "Name",
  criterion = "Criterion",
  rater = "Rater",
  items_perf = "Items...43",  
  options_perf = "Options...44",  
  m_perf = "M...45",  
  sd_perf = "SD...46",  
  reliab_perf = "Reliability Cronbach's alpha (or note)...47",  
  notes_perf = "Notes...48",
  r = "r",
  d = "d",
  other = "other",
  stats_notes = "Statistics Notes",
  interdep = "Interdependence",
  complexity = "Complexity",
  longevity = "Longevity",
  virtuality = "Virtuality",
  auth_differentiation = "Authority differentiation",
  div_climate = "Diversity climate",
  psych_safe = "Psych safety"
)


names(effect_sizes_en) <- names(effect_sizes_en) %>% str_replace("\n", " ") %>% str_squish()
names(effect_sizes_n_en) <- names(effect_sizes_n_en) %>% str_replace("\n", " ") %>% str_squish()
effect_sizes_en <- effect_sizes_en %>% select(rename_vec_en[!names(rename_vec_en)=="NULL"])
effect_sizes_n_en <- effect_sizes_n_en %>% select(rename_vec_n_en[!names(rename_vec_n_en)=="NULL"])

dataset <- effect_sizes_en %>%
  mutate(articlestudy = paste(id, study, sample, sep = "/"),
         language = "english") %>%
  group_by(articlestudy) %>%
  mutate(effect_id = row_number()) %>%
  ungroup()  %>% 
  # Disambiguate author_year
  group_by(author_year)  %>% 
  mutate(id_rank = match(id, unique(id))) %>%
  mutate(author_year = ifelse(max(id_rank) > 1, 
                               paste0(author_year, letters[id_rank]), 
                               author_year)) %>%
  ungroup() %>%
  select(-id_rank)

dataset_nen <- effect_sizes_n_en %>%
  mutate(articlestudy = paste(id, study, sample, sep = "/")) %>%
  group_by(articlestudy) %>%
  mutate(effect_id = row_number()) %>%
  ungroup()  %>% 
  # Disambiguate author_year
  group_by(author_year)  %>% 
  mutate(id_rank = match(id, unique(id))) %>%
  mutate(author_year = ifelse(max(id_rank) > 1, 
                               paste0(author_year, letters[id_rank]), 
                               author_year)) %>%
  ungroup() %>%
  select(-id_rank)


# Check data types

dataset <- dataset %>% bind_rows(dataset_nen %>% mutate(reliab_perf = as.character(reliab_perf))) %>% 
  mutate(n_teams = as.numeric(n_teams), domain = domain %>% str_replace("_", "-") %>% 
                                                           as.factor() %>% relevel(ref = "Demographic"))

dataset$pub_status[dataset$pub_status == "MA Dissertation"] <- "Masters Dissertation"
dataset$language[dataset$language == "Chinese"] <- "chinese"

```

## Sample description

### Global distribution

```{r fig.width=7, fig.height=10}
source("helpers/world_map.R")

teams <- dataset %>% 
  filter(!is.na(country), !country == "Multiple") %>% 
  mutate(country = case_when(
    country == "United States" ~ "United States of America",
    country == "Hong Kong SAR China" ~ "Hong Kong",
    TRUE ~ country
  )) %>% 
  group_by(articlestudy, country) %>% 
  # TK: Goes wrong where only sub-groups are compared
  # TK: Fix this - likely by manually checking d and OR effect sizes
  # TK: Also, adjust US outliers (patenting studies - not sure how)
  summarise(n_teams = max(n_teams), .groups = "drop") %>% 
  {
    create_world_map(.$country, .$n_teams, "teams", scale_start = 10)
  }

studies <- dataset %>% 
  filter(!is.na(country), !country == "Multiple") %>% 
  mutate(country = case_when(
    country == "United States" ~ "United States of America",
    country == "Hong Kong SAR China" ~ "Hong Kong",
    TRUE ~ country
  )) %>% 
  group_by(articlestudy, country) %>% 
  # TK: Goes wrong where only sub-groups are compared
  # TK: Fix this - likely by manually checking d and OR effect sizes
  # TK: Also, adjust US outliers (patenting studies - not sure how)
  summarise(studies = 1, .groups = "drop") %>% 
  {
    create_world_map(.$country, .$studies, "studies")
  }

omitted <- dataset %>% 
  filter(is.na(country) | country == "Multiple") %>% 
  mutate(status = ifelse(is.na(country), "unknown", "multiple")) %>% 
  group_by(status, articlestudy) %>% 
  summarise(n_teams = max(n_teams), n_studies = 1) %>% 
  summarise(n_teams = sum(n_teams), n_studies = sum(n_studies))

teams / studies + plot_annotation(
  caption = glue("The maps exclude {omitted[omitted$status == 'multiple', 'n_studies']} studies ({scales::comma(round(omitted[omitted$status == 'multiple', 'n_teams'])[[1]])} teams) covering multiple countries, and {omitted[omitted$status == 'unknown', 'n_studies']} studies ({scales::comma(round(omitted[omitted$status == 'unknown', 'n_teams'])[[1]])} teams) where the country was not reported.") %>% str_wrap(60),
  tag_levels = "A")
          

```

### Distribution over time

```{r fig.height=7}
studies <- dataset %>% group_by(year, domain, articlestudy) %>% summarise(n = 1, .groups = "drop") %>% mutate(type = "Samples")
studies_tot <- dataset %>% group_by(year, articlestudy) %>% summarise(n = 1, .groups = "drop") %>% mutate(type = "Samples", domain = "Total")
effects <- dataset %>% group_by(year, domain, articlestudy) %>% summarise(n = n(), .groups = "drop") %>% mutate(type = "Effects")
effects_tot <- dataset %>% group_by(year, articlestudy) %>% summarise(n = n(), .groups = "drop") %>% mutate(type = "Effects", domain = "Total")

stud_plot <- bind_rows(studies, studies_tot) %>% 
  ggplot(aes(x = year, y = n, fill = domain)) +
  geom_col() +
  facet_grid(domain ~ type, switch = "y") + 
  labs(caption = "NB: The number of total samples does not equal the sum of domain-specific studies, as many samples encompass multiple domains." %>% str_wrap(50))
 

eff_plot <- bind_rows(effects, effects_tot) %>% 
  ggplot(aes(x = year, y = n, fill = domain)) +
  geom_col() +
  facet_grid(domain ~ type, switch = "y") +
  labs(caption = "NB: The outlier number of Cognitive effects in 2022 is due to Qamar & Malik (2022) who assessed  personality traits across project stages (70 effect sizes)."  %>% str_wrap(50))

stud_plot + eff_plot & 
  jtools::theme_apa() & 
  theme(legend.position = "none", strip.placement = "outside", plot.caption = element_text(hjust = 0)) &
   labs(x = "", y = "")
```
### Distribution over sub-domains

```{r}
# Code adapted from https://github.com/nilsreimer/ironic-effects-meta-analysis
# Thanks to Nils Reimer!

fig_dat <- dataset %>% 
  group_by(domain, sub_dom, articlestudy) %>% 
  summarise(n = 1, .groups = "drop_last") %>% 
  summarise(tot = n(), .groups = "drop_last") %>%
  mutate(share = tot/sum(tot), sub_dom = str_to_title(as.character(sub_dom))) %>% 
  ungroup() %>% 
    arrange(share) %>% 
    filter(!is.na(sub_dom)) %>% 
    mutate(
      group = factor(sub_dom, levels = c("Other", sub_dom[sub_dom != "Other"] %>% unique()))
    )

fig_dat <- fig_dat %>% split(.$domain)

create_plot <- function(data, subtitle) {

data %>%
    ggplot(., aes(x = share, y = group)) +
  labs(subtitle = subtitle) +
    geom_col(
      aes(fill = if_else(group == "Other", "grey82", "black")),
      width = 0.8
    )  +
    geom_text(
      aes(
        label = tot,
        colour = if_else(share < 0.10, "black", "white"),
        hjust = if_else(share < 0.10, -0.25, 1.25)
      ),
      size = 9/.pt
    ) +
    scale_x_continuous(
      labels = scales::percent_format(accuracy = 10),
      expand = c(0, 0)
    ) +
    scale_colour_identity() + 
    scale_fill_identity() +
    theme_minimal(base_size = 10) +
    theme(
      legend.position = "none",
      plot.title = element_text(colour = "black", face = "bold"),
      axis.text = element_text(colour = "black"),
      axis.title = element_blank(),
      panel.grid = element_blank(),
      panel.grid.major.x = element_line(colour = "grey92")
    )

}

dem <- create_plot(fig_dat$Demographic, "Demographic")
cog <- create_plot(fig_dat$Cognitive, "Cognitive")
job <- create_plot(fig_dat$`Job-related`, "Job-related")


dem + cog + job + plot_annotation(caption = glue::glue("NB: Share missing to 100% are measures that combine sub-domains\n({fmt_pct(1 - sum(fig_dat$Demographic$share))} for Demographic, very rare in others.)"))
```

### Distribution over sectors & functions

```{r}
ind_sector <- dataset %>%
  select(articlestudy, ind_sector, stud_sample, setting) %>%
  distinct() %>%
  mutate(ind_sector = fct_lump_n(ind_sector, 10) %>% fct_recode("Mixed" = "Multiple/mixed")) %>%
  count(ind_sector, sort = TRUE)

df <- ind_sector %>%
  filter(!is.na(ind_sector)) %>%
  rename(group = ind_sector, tot = n) %>%
  mutate(share = tot / sum(tot) * 100, group = fct_inorder(group) %>% fct_rev()) %>%
  rename(sector = group, percentage = share)

# Calculate a truncated percentage for plotting
df$truncated_percentage <- ifelse(df$percentage > 25, 25, df$percentage)
df$truncated <- df$percentage > 25

p_sec <- df %>%
  ggplot(aes(y = sector, x = truncated_percentage, fill = if_else(sector == "Other", "grey82", "black"))) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_x_continuous(
    limits = c(0, 29.9), expand = c(0, 0), breaks = seq(0, 28, 5),
    labels = scales::percent_format(accuracy = 1, scale = 1)
  ) +
  scale_fill_identity() +
  scale_colour_identity() +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "none",
    plot.title = element_text(colour = "black", face = "bold"),
    axis.text = element_text(colour = "black"),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    panel.grid.major.x = element_line(colour = "grey92")
  ) +
  geom_segment(
    data = subset(df, percentage > 25),
    aes(y = sector, yend = sector, xend = 25, x = 24.2),
    arrow = arrow(type = "closed", length = unit(0.09, "inches")),
    inherit.aes = FALSE, color = "darkred", size = 1.5
  ) +
  geom_text(
    data = subset(df, truncated),
    aes(x = 27.7, label = paste0(round(percentage), "%")),
    vjust = .3, color = "darkred", size = 9 / .pt
  ) +
  geom_text(
    aes(
      label = tot,
      colour = if_else(percentage < 10, "black", "white"),
      hjust = if_else(percentage < 10, -0.5, 1.75)
    ),
    size = 9 / .pt
  ) +
  labs(subtitle = "Studies per industry sector")

team_function <- dataset %>%
  select(articlestudy, team_function, stud_sample, setting) %>%
  distinct() %>%
  mutate(team_function = fct_lump_n(team_function, 10)) %>%
  count(team_function, sort = TRUE)

df <- team_function %>%
  filter(!is.na(team_function)) %>%
  rename(group = team_function, tot = n) %>%
  mutate(share = tot / sum(tot) * 100, group = fct_inorder(group) %>% fct_rev()) %>%
  rename(percentage = share)

# Calculate a truncated percentage for plotting
df$truncated_percentage <- ifelse(df$percentage > 25, 25, df$percentage)
df$truncated <- df$percentage > 25

p_fun <- df %>%
  ggplot(aes(y = group, x = truncated_percentage, fill = if_else(group == "Other", "grey82", "black"))) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_x_continuous(
    limits = c(0, 29.9), expand = c(0, 0), breaks = seq(0, 28, 5),
    labels = scales::percent_format(accuracy = 1, scale = 1)
  ) +
  scale_fill_identity() +
  scale_colour_identity() +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "none",
    plot.title = element_text(colour = "black", face = "bold"),
    axis.text = element_text(colour = "black"),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    panel.grid.major.x = element_line(colour = "grey92")
  ) +
  geom_segment(
    data = subset(df, percentage > 25),
    aes(y = group, yend = group, xend = 25, x = 24.2),
    arrow = arrow(type = "closed", length = unit(0.09, "inches")),
    inherit.aes = FALSE, color = "darkred", size = 1.5
  ) +
  geom_text(
    data = subset(df, truncated),
    aes(x = 27.7, label = paste0(round(percentage), "%")),
    vjust = .3, color = "darkred", size = 9 / .pt
  ) +
  geom_text(
    aes(
      label = tot,
      colour = if_else(percentage < 10, "black", "white"),
      hjust = if_else(percentage < 10, -0.5, 1.75)
    ),
    size = 9 / .pt
  ) +
  labs(subtitle = "Studies per function")


p_sec + p_fun + plot_annotation(caption = glue::glue("NB: Share missing to 100% are missing values,\nlargely from business simulations and experiments\n({fmt_pct(dataset %>% select(articlestudy, team_function) %>% 
  distinct() %>% summarise(mean(is.na(ind_sector))) %>% pull())} for sectors & {fmt_pct(dataset %>% select(articlestudy, team_function) %>% 
  distinct() %>% summarise(mean(is.na(team_function))) %>% pull())} for functions)."))


```

### Distribution over moderators

Explain omissions:
- Diversity climate & psychological safety rarely reported (< 5%) - and if so, then always as positive

Explain NAs:
- 

```{r}

variables <- c("art_focus", "pub_status", "interdep", "complexity", "tmt", "stud_sample",
               "meas_type", "design", "rater", "virtuality", "auth_diff", "language")

var_names <-  tibble::tribble(
  ~old,           ~new,           
   "art_focus",    "Article focus",   
   "pub_status",   "Publication status",  
   "language",      "Language",      
   "design",       "Design",      
   "tmt",          "TMT",         
   "stud_sample",  "Student sample", 
   "meas_type",    "Diversity measure",   
   "rater",        "Performance rater",       
   "interdep",     "Interdependence",    
   "complexity",   "Complexity",  
   "virtuality",   "Virtuality",  
   "auth_diff",    "Authority Differentiation"   
)

level_names <- tibble::tribble(
  ~var,           ~level_old,                 ~level_new,                 
   "art_focus",    "focal H",                  "Focal hypothesis",                 
   "art_focus",    "auxiliary H",              "Auxiliary hypothesis",             
   "art_focus",    "descriptive",              "Descriptive",             
   "pub_status",   "Published",                "Published",               
   "pub_status",   "Masters Dissertation",          "Masters Dissertation",         
   "pub_status",   "Working paper/Preprint",   "Working Paper/Preprint",  
   "pub_status",   "Conference presentation",  "Conference Presentation", 
   "pub_status",   "PhD Dissertation",         "PhD Dissertation",        
   "interdep",     "high",                     "High",                    
   "interdep",     "medium",                   "Medium",                  
   "interdep",     "low",                      "Low",                     
   "complexity",   "high",                     "High",                    
   "complexity",   "medium",                   "Medium",                  
   "complexity",   "low",                      "Low",                     
   "tmt",          "yes",                      "Yes",                     
   "tmt",          "no",                       "No",                      
   "stud_sample",  "yes",                      "Yes",                     
   "stud_sample",  "no",                       "No",                      
   "meas_type",    "Variety",                  "Variety",                 
   "meas_type",    "Separation",               "Separation",              
   "meas_type",    "Other",                    "Other",                   
   "design",       "Experimental",             "Experimental",            
   "design",       "Observational",            "Observational",           
   "rater",        "Objective",                "Objective",               
   "rater",        "Subjective - self",        "Subjective - Self",       
   "rater",        "Subjective - supervisor",  "Subjective - Supervisor", 
   "rater",        "Subjective - external",    "Subjective - External",   
   "virtuality",   "physical",                 "Physical",                
   "virtuality",   "hybrid-members",           "Hybrid-Members",          
   "virtuality",   "virtual",                  "Virtual",                 
   "auth_diff",    "high",                     "High",                    
   "auth_diff",    "mixed",                    "Mixed",                   
   "auth_diff",    "low",                      "Low",
   "language",  "chinese",     "Chinese",    
   "language",  "dutch",       "Dutch",      
   "language",  "english",     "English",    
   "language",  "french",      "French",     
   "language",  "german",      "German",     
   "language",  "indonesian",  "Indonesian", 
   "language",  "italian",     "Italian",    
   "language",  "japanese",    "Japanese",   
   "language",  "korean",      "Korean",     
   "language",  "portuguese",  "Portuguese", 
   "language",  "spanish",     "Spanish"
)


summarize_cat_variable <- function(dataset, variable) {
    # Domain-specific summary
    domain_summary <- dataset %>%
        select(articlestudy, all_of(variable), domain) %>%
        distinct() %>%
      rename(level = !!sym(variable)) %>% 
        group_by(domain, level) %>%
        summarise(count = n(), .groups = 'drop_last') %>%
        mutate(share = count / sum(count), variable = variable)

    # Total summary
    total_summary <- dataset %>%
        select(articlestudy, all_of(variable)) %>%
        distinct() %>%
            rename(level = !!sym(variable)) %>% 

        group_by(level) %>%
        summarise(count = n(), .groups = 'drop') %>%
        mutate(share = count / sum(count), domain = "Total", variable = variable)
    # Combine domain-specific and total summaries
    summary <- bind_rows(domain_summary, total_summary)
    
    # Pivot wider and format the count and share
    summary %>%
        mutate(count_share =  paste0(count, " (", scales::percent(share, accuracy = .1), ")")) %>%
        select(-count, -share) %>%
        tidyr::pivot_wider(names_from = domain, values_from = count_share, values_fill = "0") %>%
        select(variable, level, everything()) %>% 
      left_join(total_summary %>% select(variable, level, total_count = count))
}


# Function to rename and reorder variables and levels
apply_var_and_level_names <- function(result_table, var_names, level_names) {

  # Order
  result_table <- result_table %>%
    arrange(match(variable, var_names$old),
            match(paste(variable, level), paste(level_names$var, level_names$level_old)))
  
    # Renaming levels
    result_table <- result_table %>%
        left_join(level_names, by = c("variable" = "var", "level" = "level_old")) %>%
        mutate(level = ifelse(is.na(level_new), level, level_new)) %>%
        select(-level_new)

        # Renaming variables
    result_table <- result_table %>%
        left_join(var_names, by = c("variable" = "old")) %>%
        mutate(variable = ifelse(is.na(new), variable, new)) %>%
        select(-new)

    
    result_table
}

result_table <- purrr::map_dfr(variables, ~summarize_cat_variable(dataset, .x))

# Apply renaming and reordering
final_table <- apply_var_and_level_names(result_table, var_names, level_names)

final_table %>% 
  gt(groupname_col = "variable", rowname_col = "level") %>%
  tab_stubhead(label = "Variable") %>%
  cols_label(total_count = "") %>% 
  tab_spanner("Diversity domain", Demographic:`Job-related`) %>% 
  sub_missing(columns = everything(), missing_text = "(missing)") %>% 
  gt_apa_style() %>% tab_style(
        style = list(cell_text(weight = "bold")),
        locations = cells_row_groups()
    ) %>% 
      tab_style(
        style = cell_text(align = "left", indent = px(15)),
        locations = cells_stub()
    ) %>% 
  tab_header("Distribution of effect sizes across moderator variables") %>% 
    gt_plt_bar(column = total_count, keep_column = FALSE, color = "grey")
  

  
```


## Estimate 'corrected' correlations and standard errors

### Effective sample sizes

Largest sample sizes associated with studies that sampled outputs produced by teams (e.g., patents, Wikipedia articles and academic publications) rather than teams. Multiple of these may be created by the same team and frequently teams will overlap. Nevertheless, they provide relevant data based on large samples, so that we wanted to include them. As there is no systematic way to estimate how many independent teams these represent, the sample sizes were windsorized to the largest sample size representing independent teams. This affected `r dataset %>% filter(n_obs == "OUTPUTS") %>% pull(id) %>% unique() %>% length()` studies.

```{r}
dataset$n_teams_coded <- dataset$n_teams

other_dataset <-  dataset %>% filter(is.na(n_obs) | n_obs != "OUTPUTS") 
max_sample <- max(other_dataset$n_teams)

# TK - keep relative weights in case of multiple sub-group comparisons?
adj_dataset <-  dataset %>% filter(n_obs == "OUTPUTS") %>% 
  rowwise() %>% 
  mutate(n_teams = min(n_teams, max_sample)) %>% 
  ungroup()

dataset <- bind_rows(other_dataset, adj_dataset)
```

Many more studies relied on multiple observations of the same teams, e.g, seasons for sports-teams and years for firms. These observations are evidently dependent, so that the number of observations cannot be treated as the effective sample size. Meta-analyses in business psychology rarely address this issue explicitly, and sometimes appear to treat observations from panel data as independent - yet that can give excessive weight to studies based on a low number of independent clusters. Instead, this needs to be corrected for the auto-correlation. This is rarely reported in the papers considered, so had to be assumed based on available data.

For sports teams, we identified two sources that reported year-on-year correlations, reporting r = .72 for the season-to-season win percentage in the NBA (Landis, 2001), and r = .64 for the season-to-season goal difference in the German Bundesliga (calculated based on Ben-Ner et al., 2017). Therefore, we assumed a year-on-year correlation of .7 for repeated observations of sports teams.

For year-on-year company performance, we consistently identified lower correlations, specifically:
- Return on assets (logged): .43 (Hambrick et al., 2014)
- Return on assets (Rickley et al., 2021): .54
Therefore, we assumed a year-on-year correlation of .5 for repeated measures of corporate and team performance.

However, for specific operational measures, the correlation is likely to be substantially different and likely higher. For instance, Pegels et al (2000) report that airline's load factor had a year-on-year correlation of .96. Similarly, Zouaghi et al. (2020) measured R&D performance every year based on whether an innovative product had been introduced in the previous 3 years, which will evidently correlate strongly due to the measurement choice, regardless of underlying autocorrelation. Therefore, we only considered the number of independent observations in such cases. 

After that, we calculated the effective sample size per observation using the common formula provided by the Stan Development Team [(2024)](https://mc-stan.org/docs/reference-manual/effective-sample-size.html), where N refers to the number of observations per observed team, r to the correlation between adjacent time-points and N* to the effective sample size per team.


??? (and minimum 1)

$$
N^* = max(\frac{N}{1 + 2 \times r}, 1)
$$


```{r}
other_dataset <- dataset %>% filter(is.na(n_obs) | str_detect(n_obs, "EXCL|OUTPUTS") | !team_function %in% c("Management", "Sports players"))

adj_dataset <- dataset %>% filter(!is.na(n_obs), !str_detect(n_obs, "EXCL|OUTPUTS"), team_function %in% c("Management", "Sports players")) %>% 
  mutate(scale = case_when(
      team_function == "Management" ~ (1 + 2 * 0.5),
      team_function == "Sports players" ~ (1 + 2 * 0.7)
      ),
         n_each = as.numeric(gsub("[^0-9.]+", "", n_obs)) / n_teams,
         n_teams = n_teams * pmax(1, n_each * scale)) %>% 
  select(-scale, -n_each)
                  
dataset <- bind_rows(other_dataset, adj_dataset)

```


### Correction for attenuation


```{r}

# Calculate r from other measures
# Formulae taken from Polanin & Snilsveit (Campbell SR, DOI: 10.4073/cmpn.2016.3)

d_to_r <- function(d, n1 = NULL, n2 = NULL, n = NULL) {
  # If only n is provided, equal group sizes are assumed (done throughout this MA)
  if (is.null(n1) && is.null(n2)) {n1 = n/2; n2 = n/2}
  a <- (n1 + n2)^2 / (n1 * n2)
  r <- d / sqrt(d^2 + a)
  return(r)
}

OR_to_r <- function(OR, n1 = NULL, n2 = NULL, n = NULL) {
  # If only n is provided, equal group sizes are assumed (done throughout this MA)
  if (is.null(n1) && is.null(n2)) {n1 = n/2; n2 = n/2}
  a <- (n1 + n2)^2 / (n1 * n2)
  r <- (log(OR) * (sqrt(3)/pi)) / sqrt((log(OR) * (sqrt(3)/pi)) + a)
  return(r)
}

calculate_es <- function(row) {
  if (!is.na(row$r)) return(row$r %>% as.numeric())
  if (!is.na(row$d)) return(d_to_r(row$d %>% as.numeric(), n = row$n_teams))
  if (!is.na(row$other)) {
    if (str_detect(row$other, "OR")) {
      return(OR_to_r(str_extract(row$other, "\\d*\\.\\d+") %>% as.numeric(), n = row$n_teams))
    } else {
      message("Challenge in ", row$id, " row: ", row$effect_id)
      return(NA)
    }
  }
}

dataset <- dataset %>% rowwise() %>% 
  mutate(r_rep = calculate_es(pick(everything()))) %>% 
  ungroup()


# Transform special reliabilities
# For 2-item scale, Cronbachs alpha is (2 * r) / (1 + r), so that can be directly converted
# Expect 'NAs introduced by coercion' warnings from case_when (https://github.com/tidyverse/dplyr/issues/6250)

dataset <- dataset %>%
  mutate(
    reliab_div_reported = reliab_div,
    reliab_perf_reported = reliab_perf,
    across(
      .cols = c(reliab_div, reliab_perf),
      .fns = list(
        type = ~case_when(
          is.na(.) ~ NA_character_,
          str_detect(., "^[0-9.]+$") ~ "cronbach",
          str_detect(., "CR") ~ "comp_reliab_other",
          str_detect(., "r =") ~ "r",
          str_detect(., "ICC|interrater|IRR") ~ "ICC_interrater",
          str_detect(., "[0-9]") ~ "other",
          TRUE ~ "other"
        ),
        conv = ~case_when(
          str_detect(., "^[0-9.]+$") ~ as.numeric(.),
          str_detect(., "CR") ~ as.numeric(str_extract(., "[0-9.]+")),
          str_detect(., "r =") ~ {
            r_value <- as.numeric(str_extract(., "(?<=r = ?)[0-9.]+"))
            (2 * r_value) / (1 + r_value)
          },
          str_detect(., "ICC|interrater|IRR") ~ as.numeric(str_extract(., "[0-9.]+")),
          str_detect(., "[0-9]") ~ as.numeric(str_extract(., "[0-9.]+")),
          TRUE ~ NA_real_
        )
      )
    )
  )

# Impute missing reliabilities
div_scale_terciles <- quantile(dataset$items_div %>% unique() %>% setdiff(c(1, NA)), c(.33, .66), na.rm = TRUE)
perf_scale_terciles <- quantile(dataset$items_perf %>% unique() %>% setdiff(c(1, NA)), c(.33, .66), na.rm = TRUE)

dataset <- dataset %>%
  mutate(div_tercile = cut(dataset$items_div, breaks = c(1, div_scale_terciles, Inf), labels = c(1, 2, 3), include.lowest = FALSE),
         perf_tercile = cut(dataset$items_perf, breaks = c(1, perf_scale_terciles, Inf), labels = c(1, 2, 3), include.lowest = FALSE)
) 

div_reliabilities <-  split(dataset$reliab_div_conv, dataset$div_tercile) %>% map(na.omit)
perf_reliabilities <- split(dataset$reliab_perf_conv, dataset$perf_tercile) %>% map(na.omit)

sample_reliab <- function(x, items, type, tercile) {
  if (!is.na(x)) return(x)
  if (is.na(items)) {cat(str_sub(type,1,1)); return(1)} # When both reliability and number of items are missing, no basis for adjustment
  if (items == 1) return(1)
  if (type == "div") {
    return(sample(div_reliabilities[[as.character(tercile)]], 1))
  }
  if (type == "perf") {
    return(sample(perf_reliabilities[[as.character(tercile)]], 1))
  }
  stop("Flow logic error in ", type)
}

#TK: consider need for team-size capping (would need to happen here)
#TK: consider whether reliabilities should come from sub-categories (for diversity) - 
# challenge: almost all cognitive, so little variance for others

set.seed(1347)

dataset <- dataset %>% rowwise() %>% 
  mutate(
  reliab_div = sample_reliab(reliab_div_conv, 
                             items_div, "div", div_tercile),
  reliab_perf = sample_reliab(reliab_perf_conv, 
                             items_perf, "perf", perf_tercile)
  ) %>% ungroup() %>% 
  mutate(r_adj = r_rep / (sqrt(reliab_div) * sqrt(reliab_div)),
         se =  sqrt((1 - r_rep^2) / (n_teams - 2)),
         r_scale = ifelse(r_adj == 0, 1, r_adj/r_rep),
         var_adj = se ^ 2 * (r_scale)^2) %>% 
  select(-r_scale)

# Cap adjusted rs to +/- 1 ... more extreme values only arise in tiny samples due to sampling error,
# and maintaining them only increases overall error

dataset <- dataset %>% mutate(r_adj = pmin(pmax(-1, r_adj), 1))

```


# Main meta-analysis

```{r main, include=FALSE}
# Estimate covariance matrix for CHE meta-analysis, following Harrer:
# https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/multilevel-ma.html#fit-rve

V <- with(dataset, 
          impute_covariance_matrix(vi = var_adj,
                                   cluster = articlestudy,
                                   r = rho))

meta_model_intercept <- rma.mv(r_adj ~ 1 + domain,
                    V = V,
                    random = ~ 1 | articlestudy/effect_id,
                    data = dataset,
                    sparse = TRUE)

meta_model <- rma.mv(r_adj ~ 0 + domain,
                    V = V,
                    random = ~ 1 | articlestudy/effect_id,
                    data = dataset,
                    sparse = TRUE)


confidence_intervals <- conf_int(meta_model, vcov = "CR2", p_values = TRUE)

# Equivalence testing not implemented with RVE, but equivalent to inverting confidence intervals 
# (Campbell, 2023: https://arxiv.org/abs/2004.01757)
# For example, we will reject the above null hypothesis (H0: βk ≤ ∆k, lower or βk ≥ ∆k, upper), at a α significance level,
# whenever a (1-2α)% CI for βk fits entirely within (∆k, lower, ∆k, upper).
# To continue using RVE, we will do that here by searching for the smallest p-value for which this is true
# p-value for test can be max(p-lower, p-upper) - so for symetric CIs and bounds, only need to consider one direction

get_p_value <- function(model, coef, p_precision = .001, min_p = .0001, delta = .1) {
  
  confidence_interval <- conf_int(model, vcov = "CR2", coefs = coef, level = 1 - 2 * min_p)
  
  # Identify whether we care about lower or upper bound
  direction <- if (confidence_interval$beta < 0) "CI_L" else "CI_U"
  
  # Test boundary conditions
  # Estimate outside interval
  if (abs(confidence_interval$beta) >= delta) return(1)
  
  # p < min_p
  if (abs(confidence_interval[[direction]]) < delta) {
    message("Test significant at min_p, so returning 0. Make sure to report as <= ", format(min_p, scientific = FALSE))
    return(0)
  }
  
  # p >= .5
  confidence_interval <- conf_int(model, vcov = "CR2", coefs = coef, level = .001)
  if (abs(confidence_interval[[direction]]) > delta) {
    warning("Test not significant at *p* = .495. One-sided tests cannot identify two-sided p-value above .5, so returning NA")
    return(NA)
  }
  
  alpha_high <- .495
  alpha_low <- min_p
  
  while (alpha_high - alpha_low > p_precision) {
    alpha_mid <- (alpha_high + alpha_low) / 2
    confidence_interval <- conf_int(model, vcov = "CR2", coefs = coef, level = 1 - 2 * alpha_mid)
    
    if (abs(confidence_interval[[direction]]) < delta) {
            alpha_high <- alpha_mid
    } else {
      alpha_low <- alpha_mid
    }
    }

    return(alpha_high)
  
}

confidence_intervals <- confidence_intervals %>% rowwise() %>% 
  mutate(`equiv_.1` = get_p_value(meta_model, Coef, delta = .1),
         `equiv_.05` = get_p_value(meta_model, Coef, delta = .05)) %>% 
  ungroup()
```

## Report average effects and heterogeneity

```{r results='hide'}
{
meta_summary <- summary(meta_model) %>% capture.output()
meta_summary_intercept <- summary(meta_model_intercept)  %>% capture.output()
}

I2s <- mlm.variance.distribution(meta_model)

# Calculate credibility intervals, following James Pustejovsky
# https://stat.ethz.ch/pipermail/r-sig-meta-analysis/2019-April/001508.html

cred_upper <- confidence_intervals$beta + 1.282 * sqrt(sum(meta_model$sigma2) + confidence_intervals$SE^2)
cred_lower <- confidence_intervals$beta - 1.282 * sqrt(sum(meta_model$sigma2) + confidence_intervals$SE^2)
```


```{r}
confidence_intervals %>% 
  mutate(Domain = Coef %>% str_remove("domain") %>% str_replace("_", "-"),
         `*r*` = paste(fmt_cor(beta, 3), fmt_ci(CI_L, CI_U, 3), sigstars(p_val)),
         `|*r*| < .1` = fmt_p(`equiv_.1`),
         `|*r*| < .05` = fmt_p(`equiv_.05`),
         `Credibility\ninterval` = fmt_ci(cred_lower, cred_upper, 3)) %>% 
  select((ncol(.)-4):ncol(.)) %>% 
  gt() %>% 
  fmt_labels_md() %>% 
  tab_source_note(md(paste0("\\", timesaveR:::.make_stars_note(timesaveR:::std_stars[-1])))) %>% 
  gt_apa_style()
  
```

RQ1a: Is the link between diversity and team performance insubstantial (i.e., |r| < .1)? Does this differ between the dimensions of diversity?

Average effects significantly differed between diversity domains, `r meta_summary_intercept[str_detect(meta_summary_intercept, "^QM")]` (though not substantially so? TK - run equivalence test for this?)
After accounting for that, significant amount of heterogeneity remained, `r meta_summary[str_detect(meta_summary, "^QE")]`

`r fmt_pct(as.numeric(I2s$result$I2[3])/100)` of the total variance in effect sizes can be attributed to study-level heterogeneity (I<sup>2</sup><sub>Level 3</sub>), while `r fmt_pct(as.numeric(I2s$result$I2[2])/100)` can be attributed to heterogeneity between the effects studied (I<sup>2</sup><sub>Level 2</sub>).

The 80% credibility intervals were very wide (Table x), so that substantial positive and negative associations can be regularly expected. Therefore, moderation analyses were crucial.

## Distribution of effect sizes

```{r}

```




# Meta-regression


## Prepare moderators

```{r}
if (!file.exists("data/hofstede.csv")) {
  download.file("https://geerthofstede.com/wp-content/uploads/2016/08/6-dimensions-for-website-2015-08-16.csv", "data/hofstede.csv")
}

hofstede <- read_delim("data/hofstede.csv", delim = ";", na = "#NULL!", show_col_types = FALSE) %>% 
  transmute(country, power_distance = pdi, collectivism = 100 - idv)

# Some names in Hofstede differ
# Data for Sri Lanka & Kazakhstan not available, so remains missing
hofstede_map <- c(
  "United States" = "U.S.A.", 
  "Hong Kong SAR China" = "Hong Kong", 
  "South Korea" = "Korea South", 
  "United Kingdom" = "Great Britain",
  "Cameroon" = "Africa West",  # where only regional data is available, used that
  "United Arab Emirates" = "Arab countries",  
  "Kuwait" = "Arab countries"
)

#TK: consider adding regional composites for few-country samples?

dataset <- dataset %>% 
  mutate(hofstede_match = coalesce(hofstede_map[country], country)) %>% 
  left_join(hofstede, by = c("hofstede_match" = "country")) %>% 
  select(-hofstede_match)

citation_counts <- read_excel("data/citation_data_en.xlsx") %>% distinct()

dataset <- dataset %>% 
  left_join(
    citation_counts %>% select(id = ID, citation_count)
  )

  
dataset <- dataset %>%
    mutate(year_merged = coalesce(ifelse(str_detect(year_coll, "–|-"),
                              sapply(str_split(year_coll, "–|-"), 
                                     function(x) mean(as.numeric(x))),
                              as.numeric(year_coll)),
                              year))

sum(is.na(dataset$year_merged))


# Check for redundancy ??? TKlater - not preregistered, so needs to be conservative
```

## Impute data

- Exclude variables with > 95% missing (psych safety and diversity climate)
- TK: how to deal with MNAR variables (virtuality, and likely authority diff)?

```{r}

mi_vars <- c("art_focus", "pub_status", "design", "setting", "ind_sector", "teams_function", "country", "n_teams", "stud_sample", "tmt", "domain", "sub_dom", "meas_type", "criterion", "rater", "interdep", "complexity", "longevity", "virtuality", "auth_diff", "power_distance", "collectivism", "year_merged")

convert_to_factor <- function(x) {
  if (is.character(x)) {
    return(as.factor(x))
  } else {
    return(x)
  }
}

dataset <- dataset %>% 
  mutate(across(any_of(mi_vars), convert_to_factor))

# Specify ordered factors
dataset$longevity <- factor(dataset$longevity, levels = c("hours", "days", "weeks", "months", "years", "stable"), ordered = TRUE)
dataset$interdep <- factor(dataset$interdep, levels = c("low", "medium", "high"), ordered = TRUE)
dataset$complexity <- factor(dataset$complexity, levels = c("low", "medium", "high"), ordered = TRUE)

# Get matrices
imp <- mice(dataset, maxit=0, seed = 1526)


# Set variables of interest to be predicted by each other, all others ignored
# TK: Get this to work without quickpred? Somewhat too atheoretical and ignoring multivariate context?

preds <- imp$predictorMatrix
preds[] <- 0  
pred_matrix <- dataset %>% select(all_of(mi_vars)) %>% quickpred_ext()
preds[rownames(pred_matrix), colnames(pred_matrix)] <- pred_matrix


# Standard methods per variable type are generally good (polyreg for factors, polr for ordered, pmm for continuous)  
# but polyreg does not work well with many factor levels - so these set to pmm
methods <- imp$method
methods[c("ind_sector", "teams_function", "country", "sub_dom")] <- "pmm"
methods[!names(methods) %in% mi_vars] <- ""

# Van Buren - set m to average share missing
# https://stefvanbuuren.name/fimd/sec-howmany.html
m <- dataset %>% select(all_of(mi_vars)) %>% naniar::pct_miss() %>% ceiling()
m

domains <- dataset$domain
# Ensure that sub-domains are only imputed within each domain (otherwise get invalid imputations)
mice.impute.imp_sub_domain <- function(y, ry, x, ...) {
  # y is the vector to be imputed
  # ry is a logical vector indicating which values in y are missing
  # x is the matrix of predictor variables, already filtered by predMatrix
  dots <- list(...)

  # Domain information cannot be extracted from x as it is dummy-coded there, so take from global env
  unique_domains <- unique(domains)  # Consider only domains where y is missing
  
  imputed <- rep(NA, sum(dots$wy))  # Start with y and replace missing values

  imps <- list()
  
  for (d in unique_domains) {
    # Subset data for the current domain
    rows_in_domain <- which(domains == d)  # Rows in this domain where y is missing
    y_sub <- y[rows_in_domain]
    x_sub <- x[rows_in_domain, ]
    ry_sub <- ry[rows_in_domain]
    dots_sub <- dots
    dots_sub$wy <- dots_sub$wy[rows_in_domain]
    
    # Apply polyreg imputation method for the current domain
    args_list <- c(list(y_sub, ry_sub, x_sub), dots_sub)
    imp <- do.call("mice.impute.polyreg", args_list)
    
    # Assign the imputed values back to the result vector
    #imputed[rows_in_domain] <- imp
    
    imps[d] <- list(imp)
  }
  
    # Order imps content based on domains and locations specified in wy
  ds <- domains[dots$wy]
  
    for (d in unique_domains) {
      imputed[ds == d] <- unlist(imps[d])
    }    
  
  return(imputed)
}

methods["sub_dom"] <- "imp_sub_domain"

imp <- mice(dataset, m = m, seed = 1526, predictorMatrix = preds, method = methods)
```

## Individually test moderators

Partly for consistency with earlier work, partly as we cannot determine significance of moderators (rather than individual levels) from meta-regression as we cannot presently pool Q-test (or Wald test) results (https://www.jepusto.com/mi-with-clubsandwich/)

RQ 1: Does team diversity predict team performance? How does this differ between the dimensions of diversity and the performance task under consideration?

RQ1a: Is the link between diversity and team performance insubstantial (i.e., |r| < .1)? Does this differ between the dimensions of diversity?

H1: Diversity has a substantial positive association with performance when the task is high in complexity.
H2: Diversity has a more positive association with team performance when the task requires a high level of interdependence.

H3a: Diversity has a more negative link to performance in tasks that focus on maximizing production of an output with a pre-defined strategy. (DON'T HAVE DATA?)
H3b: Diversity has a more positive link to performance in tasks where performance depends on creative divergence rather than convergence. (NEED TO DO WITH OBSERVED DATA - NO IMPUTATION)

RQ2: How does the relationship between diversity and performance differ across space and time?
RQ2a: Is the relationship between team diversity and performance related to a country’s level of collectivism versus individualism?

H4a: The relationship between diversity (particularly demographic diversity) and team performance has become more positive over time.
H4b: The relationship between diversity and team performance is positive and substantial (i.e. r > .1) in evidence from the past decade (2012-2022).

RQ3: How do contextual factors influence the relationship of diversity with team performance?
H5: Diversity has a more positive link to performance when the team works in a context that has a positive diversity climate. (DON'T HAVE DATA?)
H6: Diversity has a more positive link to performance when teams experience high levels of psychological safety. (DON'T HAVE DATA?)
H7: Diversity has a more positive link to performance when the team is low in authority differentiation than when it is high in authority differentiation. (REPLACE BY POWER DISTANCE?)

RQ3a: How does the link between diversity and performance differ depending on teams’ level of virtuality? (PROBLEMATIC DATA)

RQ3b: How does the link between diversity and performance differ depending on the longevity of a team?

RQ4: How do methodological choices influence the relationship of diversity with team performance?
RQ4a: How does the link between diversity and performance differ depending on whether performance is rated subjectively or measured objectively?
H8: Diversity will have more positive associations with performance where it is measured as variety rather than separation.
H9: Studies where the link between diversity and performance is the focal hypothesis will report larger (H8a) and more positive (H8b) effect sizes than studies where this is an auxiliary or descriptive result.

- Compare published vs unpublished ES
- Design (exp + quasi vs Observational)

Citations:
- Correlated with ES? [Cannot meaningfully assess signifiance due to prevalent repeated measures]

```{r}

moderators <- c("complexity", "interdep", "year_merged", "collectivism", "longevity", "meas_type", "rater", "design", "art_focus")

      dataset$domain <- dataset$domain %>% relevel(ref = "Demographic")

mod <- with(dataset, {
                          V <-  impute_covariance_matrix(vi = var_adj,
                                   cluster = articlestudy,
                                   r = rho)
                          
  interdep <- factor(interdep, ordered = FALSE)
  
  mod <- rma.mv(r_adj ~ 0 + interdep,
                              V = V,
                              random = ~ 1 | articlestudy/effect_id,
                              sparse = TRUE)
  
  mod_domains <- rma.mv(r_adj ~ 0 + domain + domain:interdep,
                              V = V,
                              random = ~ 1 | articlestudy/effect_id,
                              sparse = TRUE)
  
  
})


domain * year_merged + domain * complexity + domain * interdep + domain * collectivism + 
                                 domain * longevity + domain * meas_type + domain * rater + domain * design + domain * art_focus

```




## Run meta-regression

Citation count ommitted as it was registered as exploratory and logically cannot predict (precede) effect size

```{r}
# Ordered factors lead to non-sensical model outputs (polynomials)

meta_reg_models <- with(imp,
                        {
                        V <-  impute_covariance_matrix(vi = var_adj,
                                   cluster = articlestudy,
                                   r = rho)

                        longevity <- factor(longevity, ordered = FALSE) %>% relevel(ref = "stable")
                        interdep <- factor(interdep, ordered = FALSE)
                        complexity <- factor(complexity, ordered = FALSE)
                        
                        domain <- domain %>% relevel(ref = "Demographic")
                        design <- design %>% relevel(ref = "Observational")
                        rater <- rater %>% relevel(ref = "Objective")
                        
                        mod <- rma.mv(r_adj ~ 1 + domain * year_merged + domain * complexity + domain * interdep + domain * collectivism + 
                                 domain * longevity + domain * meas_type + domain * rater + domain * design + domain * art_focus,
                              V = V,
                              random = ~ 1 | articlestudy/effect_id,
                              sparse = TRUE,
                              control=list(rel.tol=1e-8))
                        
                        robu <- coef_test(mod, vcov="CR2")
                        
                        return(list(mod = mod, robu = robu))
                        })


robu_coefs <- map(meta_reg_models$analyses, "robu")
meta_reg_models_mods <- map(meta_reg_models$analyses, "mod")

# pool results with clubSandwich standard errors
# Based on Pustejovsky's 2017 blog: https://www.jepusto.com/mi-with-clubsandwich/

robust_pooled <- 
  robu_coefs %>%
  
  # add coefficient names as a column
  lapply(function(x) {
    x$coef <- row.names(x)
    x
  }) %>%
  bind_rows() %>%
  as.data.frame() %>%
  
  # summarize by coefficient
  group_by(coef) %>%
  summarise(
    m = n(),
    B = var(beta),
    beta_bar = mean(beta),
    V_bar = mean(SE^2),
    eta_bar = mean(df_Satt)
  ) %>%
  
  mutate(
    
    # calculate intermediate quantities to get df
    V_total = V_bar + B * (m + 1) / m,
    gamma = ((m + 1) / m) * B / V_total,
    df_m = (m - 1) / gamma^2,
    df_obs = eta_bar * (eta_bar + 1) * (1 - gamma) / (eta_bar + 3),
    df = 1 / (1 / df_m + 1 / df_obs),
    
    # calculate summary quantities for output
    se = sqrt(V_total),
    t = beta_bar / se,
    p_val = 2 * pt(abs(t), df = df, lower.tail = FALSE),
    crit = qt(0.975, df = df),
    lo95 = beta_bar - se * crit,
    hi95 = beta_bar + se * crit
  )

robust_pooled %>%
  select(coef, est = beta_bar, se, t, df, p_val, lo95, hi95, gamma) %>%
  mutate_at(vars(est:gamma), round, 3) %>% arrange((p_val))




            V <-  with(dataset, impute_covariance_matrix(vi = var_adj,
                                   cluster = articlestudy,
                                   r = rho))

meta_reg_obs <-  rma.mv(r_adj ~ 1 + domain * year_merged + domain * complexity + domain * interdep + domain * collectivism + 
                                 domain * longevity + domain * meas_type + domain * rater + domain * design + domain * art_focus,
                              V = V,
                        data = dataset,
                              random = ~ 1 | articlestudy/effect_id,
                              sparse = TRUE)


with(dataset, {
  complexity <- factor(complexity, ordered = FALSE)
  rma.mv(r_adj ~ 1 + complexity,
                              V = V,
                              random = ~ 1 | articlestudy/effect_id,
                              sparse = TRUE)
}) %>% summary()

pool(meta_reg_models) %>% summary() %>% arrange((p.value))


# Why not adjust SEs and then pool them?

  return(data.frame(est = coef(rma_result), se = sqrt(diag(vcov(rma_result))), robust_se = sqrt(diag(se_results$vcov))))
})

# Pool the results using Rubin's Rules
pooled_est <- colMeans(meta_regression_results$est)
within_var <- mean(meta_regression_results$se^2)
between_var <- var(meta_regression_results$est)
total_var <- within_var + (1 + 1/nrow(meta_regression_results)) * between_var
pooled_se <- sqrt(total_var)


```



# Meta-CART

## Combine ES

```{r}
datasets_single <- dataset %>% 
  group_by(articlestudy, `Publication status`, `Article  focus`, year_merged, Design, Rater, Criterion, collectivism, Interdependence, 
           Complexity, Longevity, Virtuality, `Authority  differentiation`, `Diversity climate`, `Psych safety`, Domain, 
           `Sub-domain`, `Measure type`) %>% 
  summarise(#TK - calculate linear composite) %>%
  group_by(articlestudy, Domain) %>% 
  slice_sample(n = 1) %>% 
  ungroup() %>% 
  split(.$Domain)
  
# Complete the three datasets with 1) best estimate and 2) random draw

# Single mice estimate sub-optimal due to random noise - but can polyreg/pmm fuctions be called directly?
# Alternatively, this simple hot-decking could work:


library(cluster)

# specify variables to include in hot-decking (those in metaCart and relevant other preds)
names(dataset)
hot_decking_vars <- c()

distance_matrix <- daisy(dataset  %>% select(all_of(hot_decking_vars)), metric = "gower")
hot_deck_imputation_partial <- function(data, distance_matrix) {
  for (i in 1:nrow(data)) {
    if (any(is.na(data[i,]))) {
      for (j in which(is.na(data[i,]))) {
        distances <- distance_matrix[i, -i]
        # Exclude rows that also have missing in the j-th variable
        valid_donors <- which(!is.na(data[-i, j]))
        if (length(valid_donors) > 0) {
          closest_complete_case_index <- valid_donors[which.min(distances[valid_donors])]
          data[i, j] <- data[closest_complete_case_index, j]
        }
      }
    }
  }
  return(data)
}

data_metacart_best <- dataset %>% select(all_of(hot_decking_vars))  %>% 
    hot_deck_imputation_partial(distance_matrix)


impute_with_random_draw <- function(dataframe) {
  for (col in names(dataframe)) {
    missing_indices <- which(is.na(dataframe[[col]]))
    if (length(missing_indices) > 0) {
      non_missing_values <- dataframe[[col]][!is.na(dataframe[[col]])]
      if (length(non_missing_values) > 0) {
        dataframe[[col]][missing_indices] <- sample(non_missing_values, length(missing_indices), replace = TRUE)
      }
    }
  }
  return(dataframe)
}

# Assuming 'your_dataframe' is your data frame with missing values
data_metacart_worst <- dataset %>% select(all_of(hot_decking_vars))  %>% 
    impute_with_random_draw()

```

# Publication bias

```{r}
datasets_pub <- dataset %>% 
  filter(`Publication status` == "Published") %>% 
  split(.$Domain)

# Exploratory analysis - are *claims* about diversity shaped by publication bias?
datasets_focal_pub <- dataset %>% 
  filter(`Publication status` == "Published", `Article  focus` = "focal H") %>% 
  split(.$Domain)
```


(Rodgers & Pustejovsky, 2021). In line with the simulation results and recommendations by Rodgers and Pustejovsky, we used two methods to test for publication bias. Firstly, 

## Funnel plots

```{r}
cur_data <- datasets_pub[[1]] 

# Create funnel plots with all data split by domain, coloring points grey, with `Publication status` == "Published" in mid blue and `Publication status` == "Published", `Article  focus` = "focal H" in dark blue

# Load earlier estimate?
estimate <-
sd <-

# Now, compute vectors of the lower-limit and upper limit values for
# the 95% CI region, using the range of SE that you generated in the previous step, and the stored value of your meta-analytic estimate.
se.seq=seq(0, max(cur_data$r_adj), 0.001)
ll95 = estimate-(1.96*se.seq)
ul95 = estimate+(1.96*se.seq)
 
# You can do this for a 99% CI region too
ll99 = estimate-(3.29*se.seq)
ul99 = estimate+(3.29*se.seq)
 
# And finally, do the same thing except now calculating the confidence interval
# for your meta-analytic estimate based on the stored value of its standard error
meanll95 = estimate-(1.96*se)
meanul95 = estimate+(1.96*se)
 
# Now, smash all of those calculated values into one data frame (called 'dfCI').
dfCI = data.frame(ll95, ul95, ll99, ul99, se.seq, estimate, meanll95, meanul95)
 
# Add a color column to the dataset based on the specified criteria
dataset$color <- ifelse(dataset$`Publication status` == "Published" & dataset$`Article focus` == "focal H", "dark blue",
                        ifelse(dataset$`Publication status` == "Published", "mid blue", "grey"))

# Make the funnel plot.
fp <- ggplot(aes(y = se, x = Zr), data = dataset) +
  geom_point(aes(color = color), shape = 1) +
  scale_color_identity() +
  xlab('Zr') + ylab('Standard Error') +
  geom_line(aes(y = se.seq, x = ll95), linetype = 'dotted', data = dfCI) +
  geom_line(aes(y = se.seq, x = ul95), linetype = 'dotted', data = dfCI) +
  geom_line(aes(y = se.seq, x = ll99), linetype = 'dashed', data = dfCI) +
  geom_line(aes(y = se.seq, x = ul99), linetype = 'dashed', data = dfCI) +
  geom_segment(aes(y = min(se.seq), x = meanll95, yend = max(se.seq), xend = meanll95), linetype='dotted', data=dfCI) +
  geom_segment(aes(y = min(se.seq), x = meanul95, yend = max(se.seq), xend = meanul95), linetype='dotted', data=dfCI) +
  scale_x_continuous(breaks=seq(-1.25, 2, 0.25)) +
  scale_x_reverse() +
  timesaveR::theme_apa()

```


## Egger's test

we used an Egger’s regression test to assess the asymmetry of the funnel plot, with Robust Variance Estimation (RVE) taking care of dependence between effect sizes. In order to strike an appropriate balance between statistical power and Type I errors, we followed the common practice highlighted by Siegel and colleagues (2021) and interpreted p-values below .1 as evidence for publication bias.

```{r}

datasets_published <- dataset  %>% 
    filter(pub_status == "Published")  %>%
    split(.$diversity_domain)

# Run Egger's test with Robust Variance Estimation

eggers_tests <- map(datasets_published, \(cur_dataset) {
    rho <- .6

    V <- with(cur_dataset,
            impute_covariance_matrix(vi = var_adj,
                                    cluster = articlestudy,
                                    r = rho))

    egger_multi <- rma.mv(yi = adj_cor, V = V, random = ~ 1 | articlestudy, mods = ~ adj_se, data = cur_dataset)
    coef_test(egger_multi, vcov = "CR2")
})

# TK - figure out how to display and report

```


## 3 PSM

Secondly, we used the 3-parameter selection model (3PSM) to directly estimate whether non-significant results have a lower chance of being published than significant findings. This cannot presently be extended to account for dependent effect sizes but sampling one effect size per sample results in a test that combines comparatively high power with a predictable Type I error rate. Therefore, we bootstrapped 3PSM with effect size sampling, and report the median results and distribution of 5,000 bootstrap resamples. Given that an alpha level of .05 is associated with a Type I error rate of up to 10%, we relied on this threshold (Rodgers & Pustejovsky, 2021). 

Roders & Pustejovsky (2023?) show that sampling leads to much lower Type I error rates than
ignoring dependency or aggregating effects for 3PSM

```{r}

# Define a function for 3PSM
apply_3psm <- function(data) {
    m.rma <- rma(yi = r_adj,        
                sei = se_adj,
                data = data,
                slab = authorstudy,
                method = "REML",
                test = "knha")
    psm_mod <- selmodel(m.rma,
            type = "stepfun",
            steps = 0.025)  
    # Creating a dataframe with required elements
    df <- data.frame(
        #TK - fix these once based on metafor model
        SignificanceTest = m.selmodel$pval, 
        EffectEstimateRatio = m.rma$beta / m.selmodel$beta,
        CoefNonSig = ifelse(m.selmodel$pval > 0.05, m.selmodel$beta, NA)
    )
}

# Bootstrap sampling and applying 3PSM
set.seed(123) # for reproducibility
n_bootstraps <- 1000
all_estimates <- numeric(n_bootstraps)

boot_res <- list()
for (i in 1:n_bootstraps) {
  # Sample one effect size per article/study group
  boot_sample <- do.call("rbind", lapply(split(data, data$articlestudy), function(group) {
    if (nrow(group) > 1) {
      group[sample(nrow(group), 1), ]
    } else {
      group
    }
  }))
  boot_res <- c(boot_res, apply_3psm(boot_sample))
}

boot_res <- bind_rows(boot_res)

# Calculate median estimates
# TK - do this once retun values are clear
median_estimate <- median(all_estimates)

# Visualization
hist(all_estimates, main = "Distribution of 3PSM Estimates", xlab = "Estimate")
abline(v = median_estimate, col = "red")



```


# Stat checks

## GRIM and GRIMMER

```{r}
# Add trailing 0s manually


#TK test whether trailing 0 are maintained if entered, then fix prec here
GRIMMER_div <- dataset %>% filter(!is.na(M_div)) %>% select(ID, `N teams`, Items_div:Notes_div) %>% 
  mutate(N = ifelse(!is.na(Notes_div) & str_detect(Notes_div, "N = "), str_extract(Notes_div, "\\d+") %>% as.numeric(), `N teams` %>% as.numeric()),
         across(Items_div:SD_div, as.numeric)) %>% 
  rowwise() %>% 
  mutate(GRIM = GRIM_test(M_div, N, n_items = Items_div),
         GRIMMER_std = if (GRIM) GRIMMER_test(M_div, SD_div, N, n_items = Items_div) else FALSE,
         # min_val and max_val are guessed with a simple heuristic - needs further investigation if they fail
         GRIMMER_minmax = if (GRIMMER_std) GRIMMER_test(M_div, SD_div, N, n_items = Items_div, min_val = 1, max_val = Options_div) else FALSE)

GRIMMER_perf <- dataset %>% filter(!is.na(M_perf)) %>% select(ID, `N teams`, Items_perf:Notes_perf) %>% 
  mutate(N = ifelse(!is.na(Notes_perf) & str_detect(Notes_perf, "N = "), str_extract(Notes_perf, "\\d+") %>% as.numeric(), `N teams` %>% as.numeric()),
         across(Items_perf:SD_perf, as.numeric)) %>% 
  rowwise() %>% 
  mutate(GRIM = GRIM_test(M_perf, N, n_items = Items_perf),
         GRIMMER_std = if (GRIM) GRIMMER_test(M_perf, SD_perf, N, n_items = Items_perf) else FALSE)

,
         # min_val and max_val are guessed with a simple heuristic - needs further investigation if they fail
         GRIMMER_minmax = if (GRIMMER_std) GRIMMER_test(M_perf, SD_perf, N, n_items = Items_perf, min_val = 1, max_val = Options_perf) else FALSE)



```

## Statcheck

```{r}
# Note: Replace 'dummy_statcheck_function' with actual statcheck function
run_statcheck <- function(filename) {
  file_path <- file.path("full_text", filename)
  pdf_text <- pdftools::pdf_text(file_path) |>
              paste(collapse = "") |>
              gsub("\n", "", .)

  statcheck_result <- statcheck::statcheck(pdf_text, pZeroError = FALSE, OneTailedTxt = TRUE)

  if (is.null(statcheck_result)) {
    return(NA)
  } else {
    return(any(statcheck_result$error))
  }
}
 Initialize or read existing statcheck results
if(file.exists("statcheck_results.csv")) {
  statcheck_results <- read.csv("statcheck_results.csv")
} else {
  statcheck_results <- data.frame(ID = character(), filename = character(), statcheck_issue = logical())
}

# Assuming 'ID' is a column in your dataset
# Filter out IDs already processed
dataset_filtered <- dataset %>%
  filter(!ID %in% statcheck_results$ID)

# Run statcheck for unique files in the filtered dataset
new_statcheck_results <- unique(dataset_filtered$filename) %>%
  setNames(nm = .) %>%
  map_df(~data.frame(ID = dataset_filtered$ID[dataset_filtered$filename == .x],
                     filename = .x, 
                     statcheck_issue = run_statcheck(.x)))

# Append new results to existing statcheck results and save
final_statcheck_results <- bind_rows(statcheck_results, new_statcheck_results)
write.csv(final_statcheck_results, "statcheck_results.csv", row.names = FALSE)

# Merge the statcheck results with the original dataset
dataset <- dataset %>%
  left_join(final_statcheck_results, by = "ID")

```


## Retractions


