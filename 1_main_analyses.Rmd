---
title: "Diversity and team-performance meta-analysis"
description: null
output:
  html_document:
    theme: united
    toc: yes
  html_notebook: default
  pdf_document:
    fig_height: 6
    fig_width: 8
  word_document: default
Author: Lukas Wallrich 
---

*Credits:* This code builds on the [RMarkdown template for Correlational studies meta-analysis in Psychology](https://osf.io/f85uy/), developed by Adrien Fillon and Gilad Feldman. 


```{r setup, include=FALSE}
## This block ...
## ... loads, merges and renames the dataset

if (!require(groundhog)) install.packages('groundhog')
groundhog::groundhog.library(c("readxl", "metaforest", "metafor", "tidyverse", "clubSandwich", "cli", "rsprite2", "esc",
                               "mice", "metacart", "gt", "gtExtras", "psych", "furrr", "progressr"), date = "2023-07-09")

# Need more recent version of patchwork due to faceting bug
groundhog::groundhog.library(c("sf", "rworldmap", "numform", "patchwork"), date = "2023-07-09")

groundhog::groundhog.library(c("lukaswallrich/timesaveR"), date = "2023-07-09")

# Avoid namespace clash with metaforest
coef_test <- clubSandwich::coef_test

source("helpers/helpers.R")

# Read M/SD as character to retain trailing 0s for GRIM/GRIMMER
col_types_en <- c(rep("?", 56), rep("_", 8))
col_types_en[c(3, 33:35, 42:45)] <- "c"
col_types_en <- paste0(col_types_en, collapse = "")

googlesheets4::gs4_deauth()
effect_sizes_en <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1pEYZUZvFr8qmULT077y932BUShpJvhs-asRuyyiBKlQ/edit#gid=1628922200", 
                                             sheet = "Unified Coding",
                                             skip = 2,
                                             col_types = col_types_en,
                                              na = c("#N/A", "NA", "")) %>% 
                              filter(!is.na(ID)) %>% 
  mutate(ID = str_remove(ID, "\\.0$"))

col_types_nen <- c(rep("?", 59), rep("_", 8))
col_types_nen[c(36:38, 45:48)] <- "c"
col_types_nen <- paste0(col_types_nen, collapse = "")

effect_sizes_n_en <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1XCjlC3u7Ws2KCjaRQ1R0VuU0mLQ5mHVipmzuGtivLCs/edit#gid=1553069535", 
                                             sheet = "Unified Coding",
                                             skip = 2,
                                             col_types = col_types_nen,
                                              na = c("#N/A", "NA", "")) %>% 
                              filter(!is.na(ID)) %>%  filter(is.na(Excluded) | Excluded != "yes") %>% 
    mutate(ID = str_remove(ID, "\\.0$"))


# effect_sizes_n_en <- read_excel("data/non-English_coding-WIP.xlsx",
#                                 range = cell_limits(c(3, 1), c(NA, 59),
#                                                   sheet = "Unified Coding"),
#                                 na = c("#N/A", "NA"),
#                                 col_types = col_types_nen) %>% filter(!is.na(ID)) %>%
#   filter(is.na(Excluded) | Excluded != "yes") %>% 
#   mutate(ID = str_remove(ID, "\\.0$"))


#effect_sizes_other <- read_excel("data/english_coding-WIP.xlsx")

# Assumed correlation between dependent effect sizes
rho <- 0.6

# Rename variables
rename_vec_en <- c(
  NULL = "double coded",
  id = "ID",
  effect_id = "effect_id",
  #excl = "Excluded",
  title = "Title",
  author_year = "Author (Year)",
  NULL = "File", # Only contains "PDF", not the link
  file = "File URL",
  year = "Year",
  NULL = "Coder",
  NULL = "Status",
  NULL = "Date coded",
  study = "Study",
  sample = "Sample",
  art_focus = "Article focus",
  pub_status = "Publication status",
  nonlin_rel = "Non-linear relationship",
  gen_notes =  "General Notes",
  design = "Design",
  setting = "Setting",
  ind_sector = "Industry/sector",
  team_function = "Function",
  country = "Country",
  n_teams = "N teams",
  n_obs = "N obs",
  stud_sample = "Student sample",
  tmt = "TMT",
  year_coll = "Year collected (if reported)",
  domain = "Domain",
  sub_dom = "Sub-domain",
  div_specific = "Specify",
  meas_type = "Measure type",
  items_div = "Items...31",
  opts_div = "Options...32",
  m_div = "M...33",
  sd_div = "SD...34",
  reliab_div = "Reliability Cronbach's alpha (or note)...35",
  notes_div = "Notes...36",
  name_perf = "Name",
  criterion = "Criterion",
  rater = "Rater",
  items_perf = "Items...40",
  opts_perf = "Options...41",
  m_perf = "M...42",
  sd_perf = "SD...43",
  reliab_perf = "Reliability Cronbach's alpha (or note)...44",
  notes_perf = "Notes...45",
  r = "r",
  d = "d",
  other = "other",
  stats_notes = "Statistics Notes",
  interdep = "Interdependence",
  complexity = "Complexity",
  longevity = "Longevity",
  virtuality = "Virtuality",
  auth_diff = "Authority differentiation",
  div_climate = "Diversity climate",
  psych_safe = "Psych safety"
)

rename_vec_n_en <- c(
  id = "ID",
  effect_id = "rowid",
  NULL = "Title",
  NULL = "File Or", 
  NULL = "File En",
  file = "File Or URL", # Contains original URL
  year = "Year",
    author_year = "Author (Year)",
  NULL = "Coder",
  NULL = "Status",
  NULL = "Date coded",
  study = "Study",
  sample = "Sample",
  language = "Language",
  art_focus = "Article focus",
  pub_status = "Publication status",
  nonlin_rel = "Non-linear relationship",
  gen_notes = "General Notes",
  design = "Design",
  setting = "Setting",
  ind_sector = "Industry/sector", 
  team_function = "Function",
  country = "Country",
  n_teams = "N teams",
  n_obs = "N obs", 
  stud_sample = "Student sample",
  tmt = "TMT",
  year_coll = "Year collected (if reported)",
  domain = "Domain",
  sub_dom = "Sub-domain",
  div_dom_specific = "Specify...31",  
  meas_type = "Measure type",
  div_specific = "Specify...33", 
  items_div = "Items...34",  
  options_div = "Options...35",  
  m_div = "M...36",  
  sd_div = "SD...37", 
  reliab_div = "Reliability Cronbach's alpha (or note)...38",  
  notes_div = "Notes...39",  
  name_perf = "Name",
  criterion = "Criterion",
  rater = "Rater",
  items_perf = "Items...43",  
  options_perf = "Options...44",  
  m_perf = "M...45",  
  sd_perf = "SD...46",  
  reliab_perf = "Reliability Cronbach's alpha (or note)...47",  
  notes_perf = "Notes...48",
  r = "r",
  d = "d",
  other = "other",
  stats_notes = "Statistics Notes",
  interdep = "Interdependence",
  complexity = "Complexity",
  longevity = "Longevity",
  virtuality = "Virtuality",
  auth_differentiation = "Authority differentiation",
  div_climate = "Diversity climate",
  psych_safe = "Psych safety"
)


names(effect_sizes_en) <- names(effect_sizes_en) %>% str_replace("\n", " ") %>% str_squish()
names(effect_sizes_n_en) <- names(effect_sizes_n_en) %>% str_replace("\n", " ") %>% str_squish()
effect_sizes_en <- effect_sizes_en %>% select(rename_vec_en[!names(rename_vec_en)=="NULL"])
effect_sizes_n_en <- effect_sizes_n_en %>% select(rename_vec_n_en[!names(rename_vec_n_en)=="NULL"])

dataset <- effect_sizes_en %>%
  mutate(articlestudy = paste(id, study, sample, sep = "/"),
         language = "english") %>%
  group_by(articlestudy) %>%
  mutate(effect_id = row_number()) %>%
  ungroup()  %>% 
  # Disambiguate author_year
  group_by(author_year)  %>% 
  mutate(id_rank = match(id, unique(id))) %>%
  mutate(author_year = ifelse(max(id_rank) > 1, 
                               paste0(author_year, letters[id_rank]), 
                               author_year)) %>%
  ungroup() %>%
  select(-id_rank)

dataset_nen <- effect_sizes_n_en %>%
  mutate(articlestudy = paste(id, study, sample, sep = "/")) %>%
  group_by(articlestudy) %>%
  mutate(effect_id = row_number()) %>%
  ungroup()  %>% 
  # Disambiguate author_year
  group_by(author_year)  %>% 
  mutate(id_rank = match(id, unique(id))) %>%
  mutate(author_year = ifelse(max(id_rank) > 1, 
                               paste0(author_year, letters[id_rank]), 
                               author_year)) %>%
  ungroup() %>%
  select(-id_rank)


# Check data types

dataset <- dataset %>% bind_rows(dataset_nen %>% mutate(reliab_perf = as.character(reliab_perf))) %>% 
  mutate(n_teams = as.numeric(n_teams), domain = domain %>% str_replace("_", "-") %>% 
                                                           as.factor() %>% relevel(ref = "Demographic"))

dataset$pub_status[dataset$pub_status == "MA Dissertation"] <- "Masters Dissertation"
dataset$language[dataset$language == "Chinese"] <- "chinese"

```

## Check for retractions

```{r}
# Updated last on 10 Feb 24
if (FALSE) {
  download.file("https://api.labs.crossref.org/data/retractionwatch?l.wallrich@bbk.ac.uk", "data/retractions.csv")
}

retracted <- read_csv("data/retractions.csv")

refs_en <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1pEYZUZvFr8qmULT077y932BUShpJvhs-asRuyyiBKlQ/edit#gid=1628922200", 
                                             sheet = "Refs",
                                             skip = 5)

refs_n_en <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1XCjlC3u7Ws2KCjaRQ1R0VuU0mLQ5mHVipmzuGtivLCs/edit#gid=1553069535", 
                                             sheet = "Refs",
                                             skip = 1)

# Check whether references match final sample
if (length(setdiff(dataset$id, c(refs_en$ID, refs_n_en$ID)))>0) {
  print(paste0("Missing references: ", glue::glue_collapse(setdiff(dataset$id, c(refs_en$ID, refs_n_en$ID)), ", ")))
}
if (length(setdiff(c(refs_en$ID, refs_n_en$ID), dataset$id))>0) {
  print(paste0("Excess references: ", glue::glue_collapse(setdiff(c(refs_en$ID, refs_n_en$ID), dataset$id), ", ")))
}

pdfs <- refs_en %>% select(ID, `File-name`) %>% 
  bind_rows(refs_n_en  %>% select(ID, `File-name`)) %>% 
  rename(id = ID, file_name = `File-name`) %>% 
  mutate(id = map_chr(id, \(x) c(x, NA)[1]))

intersect(refs_en$DOI %>% str_to_lower() %>% str_trim(), retracted$OriginalPaperDOI %>% str_to_lower() %>% str_trim())
intersect(refs_n_en$DOI %>% str_to_lower() %>% str_trim(), retracted$OriginalPaperDOI %>% str_to_lower() %>% str_trim())
```

None of the included articles were retracted as per Retraction Watch's database, as [available from Crossref](https://api.labs.crossref.org/data/retractionwatch?l.wallrich@bbk.ac.uk) on 10 February 2024.

## Estimate 'corrected' correlations and standard errors

### Effective sample sizes

Largest sample sizes associated with studies that sampled outputs produced by teams (e.g., patents, Wikipedia articles and academic publications) rather than teams. Multiple of these may be created by the same team and frequently teams will overlap. Nevertheless, they provide relevant data based on large samples, so that we wanted to include them. As there is no systematic way to estimate how many independent teams these represent, the sample sizes were windsorized to the largest sample size representing independent teams. This affected `r dataset %>% filter(n_obs == "OUTPUTS") %>% pull(id) %>% unique() %>% length()` studies.

```{r}
dataset$n_teams_coded <- dataset$n_teams

other_dataset <-  dataset %>% filter(is.na(n_obs) | n_obs != "OUTPUTS") 
max_sample <- max(other_dataset$n_teams)

# TK - keep relative weights in case of multiple sub-group comparisons?
adj_dataset <-  dataset %>% filter(n_obs == "OUTPUTS") %>% 
  rowwise() %>% 
  mutate(n_teams = min(n_teams, max_sample)) %>% 
  ungroup()

dataset <- bind_rows(other_dataset, adj_dataset)
```

Many more studies relied on multiple observations of the same teams, e.g, seasons for sports-teams and years for firms. These observations are evidently dependent, so that the number of observations cannot be treated as the effective sample size. Meta-analyses in business psychology rarely address this issue explicitly, and sometimes appear to treat observations from panel data as independent - yet that can give excessive weight to studies based on a low number of independent clusters. Instead, this needs to be corrected for the auto-correlation. This is rarely reported in the papers considered, so had to be assumed based on available data.

For sports teams, we identified two sources that reported year-on-year correlations, reporting r = .72 for the season-to-season win percentage in the NBA (Landis, 2001), and r = .64 for the season-to-season goal difference in the German Bundesliga (calculated based on Ben-Ner et al., 2017). Therefore, we assumed a year-on-year correlation of .7 for repeated observations of sports teams.

For year-on-year company performance, we consistently identified lower correlations, specifically:
- Return on assets (logged): .43 (Hambrick et al., 2014)
- Return on assets (Rickley et al., 2021): .54
Therefore, we assumed a year-on-year correlation of .5 for repeated measures of corporate and team performance.

However, for specific operational measures, the correlation is likely to be substantially different and likely higher. For instance, Pegels et al (2000) report that airline's load factor had a year-on-year correlation of .96. Similarly, Zouaghi et al. (2020) measured R&D performance every year based on whether an innovative product had been introduced in the previous 3 years, which will evidently correlate strongly due to the measurement choice, regardless of underlying autocorrelation. Therefore, we only considered the number of independent observations in such cases. 

After that, we calculated the effective sample size per observation using the common formula provided by the Stan Development Team [(2024)](https://mc-stan.org/docs/reference-manual/effective-sample-size.html), where N refers to the number of observations per observed team, r to the correlation between adjacent time-points and N* to the effective sample size per team.


??? (and minimum 1)

$$
N^* = max(\frac{N}{1 + 2 \times r}, 1)
$$


```{r}
other_dataset <- dataset %>% filter(is.na(n_obs) | str_detect(n_obs, "EXCL|OUTPUTS") | !team_function %in% c("Management", "Sports players"))

adj_dataset <- dataset %>% filter(!is.na(n_obs), !str_detect(n_obs, "EXCL|OUTPUTS"), team_function %in% c("Management", "Sports players")) %>% 
  mutate(scale = case_when(
      team_function == "Management" ~ 1 / (1 + 2 * 0.5),
      team_function == "Sports players" ~ 1 / (1 + 2 * 0.7)
      ),
         n_each = as.numeric(gsub("[^0-9.]+", "", n_obs)) / n_teams,
         n_teams = n_teams * pmax(1, n_each * scale)) %>% 
  select(-scale, -n_each)
                  
dataset <- bind_rows(other_dataset, adj_dataset)

```


### Correction for attenuation


```{r}

# Calculate r from other measures
# Formulae taken from Polanin & Snilsveit (Campbell SR, DOI: 10.4073/cmpn.2016.3)

d_to_r <- function(d, n1 = NULL, n2 = NULL, n = NULL) {
  # If only n is provided, equal group sizes are assumed (done throughout this MA)
  if (is.null(n1) && is.null(n2)) {n1 = n/2; n2 = n/2}
  a <- (n1 + n2)^2 / (n1 * n2)
  r <- d / sqrt(d^2 + a)
  return(r)
}

OR_to_r <- function(OR, n1 = NULL, n2 = NULL, n = NULL) {
  # If only n is provided, equal group sizes are assumed (done throughout this MA)
  if (is.null(n1) && is.null(n2)) {n1 = n/2; n2 = n/2}
  a <- (n1 + n2)^2 / (n1 * n2)
  r <- (log(OR) * (sqrt(3)/pi)) / sqrt((log(OR) * (sqrt(3)/pi)) + a)
  return(r)
}

calculate_es <- function(row) {
  if (!is.na(row$r)) return(row$r %>% as.numeric())
  if (!is.na(row$d)) return(d_to_r(row$d %>% as.numeric(), n = row$n_teams))
  if (!is.na(row$other)) {
    if (str_detect(row$other, "OR")) {
      return(OR_to_r(str_extract(row$other, "\\d*\\.\\d+") %>% as.numeric(), n = row$n_teams))
    } else {
      message("Challenge in ", row$id, " row: ", row$effect_id)
      return(NA)
    }
  }
}

dataset <- dataset %>% rowwise() %>% 
  mutate(r_rep = calculate_es(pick(everything()))) %>% 
  ungroup()


# Transform special reliabilities
# For 2-item scale, Cronbachs alpha is (2 * r) / (1 + r), so that can be directly converted
# Expect 'NAs introduced by coercion' warnings from case_when (https://github.com/tidyverse/dplyr/issues/6250)

dataset <- dataset %>%
  mutate(
    reliab_div_reported = reliab_div,
    reliab_perf_reported = reliab_perf,
    across(
      .cols = c(reliab_div, reliab_perf),
      .fns = list(
        type = ~case_when(
          is.na(.) ~ NA_character_,
          str_detect(., "^[0-9.]+$") ~ "cronbach",
          str_detect(., "CR") ~ "comp_reliab_other",
          str_detect(., "r =") ~ "r",
          str_detect(., "ICC|interrater|IRR") ~ "ICC_interrater",
          str_detect(., "[0-9]") ~ "other",
          TRUE ~ "other"
        ),
        conv = ~case_when(
          str_detect(., "^[0-9.]+$") ~ as.numeric(.),
          str_detect(., "CR") ~ as.numeric(str_extract(., "[0-9.]+")),
          str_detect(., "r =") ~ {
            r_value <- as.numeric(str_extract(., "(?<=r = ?)[0-9.]+"))
            (2 * r_value) / (1 + r_value)
          },
          str_detect(., "ICC|interrater|IRR") ~ as.numeric(str_extract(., "[0-9.]+")),
          str_detect(., "[0-9]") ~ as.numeric(str_extract(., "[0-9.]+")),
          TRUE ~ NA_real_
        )
      )
    )
  )

# Impute missing reliabilities
div_scale_terciles <- quantile(dataset$items_div %>% unique() %>% setdiff(c(1, NA)), c(.33, .66), na.rm = TRUE)
perf_scale_terciles <- quantile(dataset$items_perf %>% unique() %>% setdiff(c(1, NA)), c(.33, .66), na.rm = TRUE)

dataset <- dataset %>%
  mutate(div_tercile = cut(dataset$items_div, breaks = c(1, div_scale_terciles, Inf), labels = c(1, 2, 3), include.lowest = FALSE),
         perf_tercile = cut(dataset$items_perf, breaks = c(1, perf_scale_terciles, Inf), labels = c(1, 2, 3), include.lowest = FALSE)
) 

div_reliabilities <-  split(dataset$reliab_div_conv, dataset$div_tercile) %>% map(na.omit)
perf_reliabilities <- split(dataset$reliab_perf_conv, dataset$perf_tercile) %>% map(na.omit)

sample_reliab <- function(x, items, type, tercile) {
  if (!is.na(x)) return(x)
  if (is.na(items)) {cat(str_sub(type,1,1)); return(1)} # When both reliability and number of items are missing, no basis for adjustment
  if (items == 1) return(1)
  if (type == "div") {
    return(sample(div_reliabilities[[as.character(tercile)]], 1))
  }
  if (type == "perf") {
    return(sample(perf_reliabilities[[as.character(tercile)]], 1))
  }
  stop("Flow logic error in ", type)
}

#TK: consider need for team-size capping (would need to happen here)
#TK: consider whether reliabilities should come from sub-categories (for diversity) - 
# challenge: almost all cognitive, so little variance for others

set.seed(1347)

dataset <- dataset %>% rowwise() %>% 
  mutate(
  reliab_div = sample_reliab(reliab_div_conv, 
                             items_div, "div", div_tercile),
  reliab_perf = sample_reliab(reliab_perf_conv, 
                             items_perf, "perf", perf_tercile)
  ) %>% ungroup() %>% 
  mutate(r_adj = r_rep / (sqrt(reliab_div) * sqrt(reliab_div)),
         se =  sqrt((1 - r_rep^2) / (n_teams - 2)),
         r_scale = ifelse(r_adj == 0, 1, r_adj/r_rep),
         var_adj = se ^ 2 * (r_scale)^2) %>% 
  select(-r_scale)

# Cap adjusted rs to +/- 1 ... more extreme values only arise in tiny samples due to sampling error,
# and maintaining them only increases overall error

dataset <- dataset %>% mutate(r_adj = pmin(pmax(-1, r_adj), 1))

```
## Add moderators

```{r add-moderator-data}
if (!file.exists("data/hofstede.csv")) {
  download.file("https://geerthofstede.com/wp-content/uploads/2016/08/6-dimensions-for-website-2015-08-16.csv", "data/hofstede.csv")
}

hofstede <- read_delim("data/hofstede.csv", delim = ";", na = "#NULL!", show_col_types = FALSE) %>% 
  transmute(country, power_distance = pdi, collectivism = 100 - idv)

# Some names in Hofstede differ
# Data for Sri Lanka & Kazakhstan not available, so remains missing
hofstede_map <- c(
  "United States" = "U.S.A.", 
  "Hong Kong SAR China" = "Hong Kong", 
  "South Korea" = "Korea South", 
  "United Kingdom" = "Great Britain",
  "Cameroon" = "Africa West",  # where only regional data is available, used that
  "United Arab Emirates" = "Arab countries",  
  "Kuwait" = "Arab countries"
)

#TK: consider adding regional composites for few-country samples?

dataset <- dataset %>% 
  mutate(hofstede_match = ifelse(country %in% names(hofstede_map), hofstede_map[country %>% as.character()], country %>% as.character()) %>%  factor()) %>% 
  left_join(hofstede, by = c("hofstede_match" = "country")) %>% 
  select(-hofstede_match)

citation_counts <- bind_rows(read_excel("data/citation_data_en.xlsx") %>% distinct(),
                             read_excel("data/citation_data_nen.xlsx") %>% distinct()) %>% 
  mutate(citation_count = coalesce(citation_count, citations))
                             

dataset <- dataset %>% 
  left_join(
    citation_counts %>% select(id = ID, citation_count)
  )

# NAs in citation count indicate that these entries could not be found on Google Scholar - thus they have 0 Google Scholar citations.
dataset <- dataset %>% 
  mutate(citation_count = replace_na(citation_count, 0))


if (is.list(dataset$year_coll)) {
  dataset <- dataset %>%
      mutate(year_coll = null_to_NA(year_coll))
} 
  dataset <- dataset %>%
      mutate(year_coll_mean = ifelse(str_detect(year_coll, "–|-"),
                                sapply(str_split(year_coll, "–|-"), 
                                       function(x) mean(as.numeric(x))),
                                as.numeric(year_coll)),
             year_merged = coalesce(year_coll_mean,
                                year))
  
```

# Sample description

### Global distribution

```{r fig.width=7, fig.height=10}
source("helpers/world_map.R")

teams <- dataset %>% 
  filter(!is.na(country), !country == "Multiple") %>% 
  mutate(country = case_when(
    country == "United States" ~ "United States of America",
    country == "Hong Kong SAR China" ~ "Hong Kong",
    TRUE ~ country
  )) %>% 
  group_by(articlestudy, country) %>% 
  # TK: Goes wrong where only sub-groups are compared
  # TK: Fix this - likely by manually checking d and OR effect sizes
  # TK: Also, adjust US outliers (patenting studies - not sure how)
  summarise(n_teams = max(n_teams), .groups = "drop") %>% 
  {
    create_world_map(.$country, .$n_teams, "teams", scale_start = 10)
  }

studies <- dataset %>% 
  filter(!is.na(country), !country == "Multiple") %>% 
  mutate(country = case_when(
    country == "United States" ~ "United States of America",
    country == "Hong Kong SAR China" ~ "Hong Kong",
    TRUE ~ country
  )) %>% 
  group_by(articlestudy, country) %>% 
  # TK: Goes wrong where only sub-groups are compared
  # TK: Fix this - likely by manually checking d and OR effect sizes
  # TK: Also, adjust US outliers (patenting studies - not sure how)
  summarise(studies = 1, .groups = "drop") %>% 
  {
    create_world_map(.$country, .$studies, "studies")
  }

omitted <- dataset %>% 
  filter(is.na(country) | country == "Multiple") %>% 
  mutate(status = ifelse(is.na(country), "unknown", "multiple")) %>% 
  group_by(status, articlestudy) %>% 
  summarise(n_teams = max(n_teams), n_studies = 1) %>% 
  summarise(n_teams = sum(n_teams), n_studies = sum(n_studies))

teams / studies + plot_annotation(
  caption = glue("The maps exclude {omitted[omitted$status == 'multiple', 'n_studies']} studies ({scales::comma(round(omitted[omitted$status == 'multiple', 'n_teams'])[[1]])} teams) covering multiple countries, and {omitted[omitted$status == 'unknown', 'n_studies']} studies ({scales::comma(round(omitted[omitted$status == 'unknown', 'n_teams'])[[1]])} teams) where the country was not reported.") %>% str_wrap(60),
  tag_levels = "A")
          

```

### Distribution over time

```{r fig.height=7}
studies <- dataset %>% group_by(year, domain, articlestudy) %>% summarise(n = 1, .groups = "drop") %>% mutate(type = "Samples")
studies_tot <- dataset %>% group_by(year, articlestudy) %>% summarise(n = 1, .groups = "drop") %>% mutate(type = "Samples", domain = "Total")
effects <- dataset %>% group_by(year, domain, articlestudy) %>% summarise(n = n(), .groups = "drop") %>% mutate(type = "Effects")
effects_tot <- dataset %>% group_by(year, articlestudy) %>% summarise(n = n(), .groups = "drop") %>% mutate(type = "Effects", domain = "Total")

stud_plot <- bind_rows(studies, studies_tot) %>% 
  ggplot(aes(x = year, y = n, fill = domain)) +
  geom_col() +
  facet_grid(domain ~ type, switch = "y") + 
  labs(caption = "NB: The number of total samples does not equal the sum of domain-specific studies, as many samples encompass multiple domains." %>% str_wrap(50))
 

eff_plot <- bind_rows(effects, effects_tot) %>% 
  ggplot(aes(x = year, y = n, fill = domain)) +
  geom_col() +
  facet_grid(domain ~ type, switch = "y") +
  labs(caption = "NB: The outlier number of Cognitive effects in 2022 is due to Qamar & Malik (2022) who assessed  personality traits across project stages (70 effect sizes)."  %>% str_wrap(50))

stud_plot + eff_plot & 
  jtools::theme_apa() & 
  theme(legend.position = "none", strip.placement = "outside", plot.caption = element_text(hjust = 0)) &
   labs(x = "", y = "")
```
### Distribution over sub-domains

```{r}
# Code adapted from https://github.com/nilsreimer/ironic-effects-meta-analysis
# Thanks to Nils Reimer!

fig_dat <- dataset %>% 
  group_by(domain, sub_dom, articlestudy) %>% 
  summarise(n = 1, .groups = "drop_last") %>% 
  summarise(tot = n(), .groups = "drop_last") %>%
  mutate(share = tot/sum(tot), sub_dom = str_to_title(as.character(sub_dom))) %>% 
  ungroup() %>% 
    arrange(share) %>% 
    filter(!is.na(sub_dom)) %>% 
    mutate(
      group = factor(sub_dom, levels = c("Other", sub_dom[sub_dom != "Other"] %>% unique()))
    )

fig_dat <- fig_dat %>% split(.$domain)

create_plot <- function(data, subtitle) {

data %>%
    ggplot(., aes(x = share, y = group)) +
  labs(subtitle = subtitle) +
    geom_col(
      aes(fill = if_else(group == "Other", "grey82", "black")),
      width = 0.8
    )  +
    geom_text(
      aes(
        label = tot,
        colour = if_else(share < 0.10, "black", "white"),
        hjust = if_else(share < 0.10, -0.25, 1.25)
      ),
      size = 9/.pt
    ) +
    scale_x_continuous(
      labels = scales::percent_format(accuracy = 10),
      expand = c(0, 0)
    ) +
    scale_colour_identity() + 
    scale_fill_identity() +
    theme_minimal(base_size = 10) +
    theme(
      legend.position = "none",
      plot.title = element_text(colour = "black", face = "bold"),
      axis.text = element_text(colour = "black"),
      axis.title = element_blank(),
      panel.grid = element_blank(),
      panel.grid.major.x = element_line(colour = "grey92")
    )

}

dem <- create_plot(fig_dat$Demographic, "Demographic")
cog <- create_plot(fig_dat$Cognitive, "Cognitive")
job <- create_plot(fig_dat$`Job-related`, "Job-related")


dem + cog + job + plot_annotation(caption = glue::glue("NB: Share missing to 100% are measures that combine sub-domains\n({fmt_pct(1 - sum(fig_dat$Demographic$share))} for Demographic, very rare in others.)"))
```

### Distribution over sectors & functions

```{r}
ind_sector <- dataset %>%
  select(articlestudy, ind_sector, stud_sample, setting) %>%
  distinct() %>%
  mutate(ind_sector = fct_lump_n(ind_sector, 10) %>% fct_recode("Mixed" = "Multiple/mixed")) %>%
  count(ind_sector, sort = TRUE)

df <- ind_sector %>%
  filter(!is.na(ind_sector)) %>%
  rename(group = ind_sector, tot = n) %>%
  mutate(share = tot / sum(tot) * 100, group = fct_inorder(group) %>% fct_rev()) %>%
  rename(sector = group, percentage = share)

# Calculate a truncated percentage for plotting
df$truncated_percentage <- ifelse(df$percentage > 25, 25, df$percentage)
df$truncated <- df$percentage > 25

p_sec <- df %>%
  ggplot(aes(y = sector, x = truncated_percentage, fill = if_else(sector == "Other", "grey82", "black"))) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_x_continuous(
    limits = c(0, 29.9), expand = c(0, 0), breaks = seq(0, 28, 5),
    labels = scales::percent_format(accuracy = 1, scale = 1)
  ) +
  scale_fill_identity() +
  scale_colour_identity() +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "none",
    plot.title = element_text(colour = "black", face = "bold"),
    axis.text = element_text(colour = "black"),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    panel.grid.major.x = element_line(colour = "grey92")
  ) +
  geom_segment(
    data = subset(df, percentage > 25),
    aes(y = sector, yend = sector, xend = 25, x = 24.2),
    arrow = arrow(type = "closed", length = unit(0.09, "inches")),
    inherit.aes = FALSE, color = "darkred", size = 1.5
  ) +
  geom_text(
    data = subset(df, truncated),
    aes(x = 27.7, label = paste0(round(percentage), "%")),
    vjust = .3, color = "darkred", size = 9 / .pt
  ) +
  geom_text(
    aes(
      label = tot,
      colour = if_else(percentage < 10, "black", "white"),
      hjust = if_else(percentage < 10, -0.5, 1.75)
    ),
    size = 9 / .pt
  ) +
  labs(subtitle = "Studies per industry sector")

team_function <- dataset %>%
  select(articlestudy, team_function, stud_sample, setting) %>%
  distinct() %>%
  mutate(team_function = fct_lump_n(team_function, 10)) %>%
  count(team_function, sort = TRUE)

df <- team_function %>%
  filter(!is.na(team_function)) %>%
  rename(group = team_function, tot = n) %>%
  mutate(share = tot / sum(tot) * 100, group = fct_inorder(group) %>% fct_rev()) %>%
  rename(percentage = share)

# Calculate a truncated percentage for plotting
df$truncated_percentage <- ifelse(df$percentage > 25, 25, df$percentage)
df$truncated <- df$percentage > 25

p_fun <- df %>%
  ggplot(aes(y = group, x = truncated_percentage, fill = if_else(group == "Other", "grey82", "black"))) +
  geom_bar(stat = "identity", width = 0.8) +
  scale_x_continuous(
    limits = c(0, 29.9), expand = c(0, 0), breaks = seq(0, 28, 5),
    labels = scales::percent_format(accuracy = 1, scale = 1)
  ) +
  scale_fill_identity() +
  scale_colour_identity() +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "none",
    plot.title = element_text(colour = "black", face = "bold"),
    axis.text = element_text(colour = "black"),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    panel.grid.major.x = element_line(colour = "grey92")
  ) +
  geom_segment(
    data = subset(df, percentage > 25),
    aes(y = group, yend = group, xend = 25, x = 24.2),
    arrow = arrow(type = "closed", length = unit(0.09, "inches")),
    inherit.aes = FALSE, color = "darkred", size = 1.5
  ) +
  geom_text(
    data = subset(df, truncated),
    aes(x = 27.7, label = paste0(round(percentage), "%")),
    vjust = .3, color = "darkred", size = 9 / .pt
  ) +
  geom_text(
    aes(
      label = tot,
      colour = if_else(percentage < 10, "black", "white"),
      hjust = if_else(percentage < 10, -0.5, 1.75)
    ),
    size = 9 / .pt
  ) +
  labs(subtitle = "Studies per function")


p_sec + p_fun + plot_annotation(caption = glue::glue("NB: Share missing to 100% are missing values,\nlargely from business simulations and experiments\n({fmt_pct(dataset %>% select(articlestudy, team_function) %>% 
  distinct() %>% summarise(mean(is.na(ind_sector))) %>% pull())} for sectors & {fmt_pct(dataset %>% select(articlestudy, team_function) %>% 
  distinct() %>% summarise(mean(is.na(team_function))) %>% pull())} for functions)."))


```

### Distribution over moderators

Explain omissions:
- Diversity climate & psychological safety rarely reported (< 5%) - and if so, then always as positive

Explain NAs:
- 

```{r}

variables <- c("art_focus", "pub_status", "interdep", "complexity", "longevity", "tmt", "stud_sample",
               "meas_type", "design", "rater", "virtuality", "auth_diff", "language")

var_names <-  tibble::tribble(
  ~old,           ~new,           
   "art_focus",    "Article focus",   
   "pub_status",   "Publication status",  
   "language",      "Language",      
   "design",       "Design",      
   "tmt",          "TMT",         
   "stud_sample",  "Student sample", 
   "meas_type",    "Diversity measure",   
   "rater",        "Performance rater",       
   "interdep",     "Interdependence",    
   "complexity",   "Complexity",  
   "virtuality",   "Virtuality",  
   "longevity",    "Longevity",
   "auth_diff",    "Authority Differentiation",   
   "collectivism",  "Collectivism",   
   "power_distance",    "Power distance",   
   "year_merged",    "Year of data collection"   
)

level_names <- tibble::tribble(
  ~var,           ~level_old,                 ~level_new,                 
   "art_focus",    "focal H",                  "Focal hypothesis",                 
   "art_focus",    "auxiliary H",              "Auxiliary hypothesis",             
   "art_focus",    "descriptive",              "Descriptive",             
   "pub_status",   "Published",                "Published",               
   "pub_status",   "Masters Dissertation",          "Masters Dissertation",         
   "pub_status",   "Working paper/Preprint",   "Working Paper/Preprint",  
   "pub_status",   "Conference presentation",  "Conference Presentation", 
   "pub_status",   "PhD Dissertation",         "PhD Dissertation",        
   "interdep",     "high",                     "High",                    
   "interdep",     "medium",                   "Medium",                  
   "interdep",     "low",                      "Low",                     
   "complexity",   "high",                     "High",                    
   "complexity",   "medium",                   "Medium",                  
   "complexity",   "low",                      "Low",                     
   "tmt",          "yes",                      "Yes",                     
   "tmt",          "no",                       "No",                      
   "stud_sample",  "yes",                      "Yes",                     
   "stud_sample",  "no",                       "No",                      
   "meas_type",    "Variety",                  "Variety",                 
   "meas_type",    "Separation",               "Separation",              
   "meas_type",    "Other",                    "Other",                   
   "design",       "Experimental",             "Experimental",            
   "design",       "Observational",            "Observational",           
   "rater",        "Objective",                "Objective",               
   "rater",        "Subjective - self",        "Subjective - Self",       
   "rater",        "Subjective - supervisor",  "Subjective - Supervisor", 
   "rater",        "Subjective - external",    "Subjective - External",   
   "virtuality",   "physical",                 "Physical",                
   "virtuality",   "hybrid-members",           "Hybrid-Members",          
   "virtuality",   "virtual",                  "Virtual",                 
   "auth_diff",    "high",                     "High",                    
   "auth_diff",    "mixed",                    "Mixed",                   
   "auth_diff",    "low",                      "Low",
   "language",  "chinese",     "Chinese",    
   "language",  "dutch",       "Dutch",      
   "language",  "english",     "English",    
   "language",  "french",      "French",     
   "language",  "german",      "German",     
   "language",  "indonesian",  "Indonesian", 
   "language",  "italian",     "Italian",    
   "language",  "japanese",    "Japanese",   
   "language",  "korean",      "Korean",     
   "language",  "portuguese",  "Portuguese", 
   "language",  "spanish",     "Spanish",
   "longevity",  "hours",    "Hours",   
   "longevity",  "days",     "Days",    
   "longevity",  "weeks",    "Weeks",   
   "longevity",  "months",   "Months",  
   "longevity",  "years",    "Years",
   "longevity",  "stable",   "Stable"  
)


summarize_cat_variable <- function(dataset, variable) {
    # Domain-specific summary
    domain_summary <- dataset %>%
        select(articlestudy, all_of(variable), domain) %>%
        distinct() %>%
      rename(level = !!sym(variable)) %>% 
        group_by(domain, level) %>%
        summarise(count = n(), .groups = 'drop_last') %>%
        mutate(share = count / sum(count), variable = variable)

    # Total summary
    total_summary <- dataset %>%
        select(articlestudy, all_of(variable)) %>%
        distinct() %>%
            rename(level = !!sym(variable)) %>% 

        group_by(level) %>%
        summarise(count = n(), .groups = 'drop') %>%
        mutate(share = count / sum(count), domain = "Total", variable = variable)
    # Combine domain-specific and total summaries
    summary <- bind_rows(domain_summary, total_summary)
    
    # Pivot wider and format the count and share
    summary %>%
        mutate(count_share =  paste0(count, " (", scales::percent(share, accuracy = .1), ")")) %>%
        select(-count, -share) %>%
        tidyr::pivot_wider(names_from = domain, values_from = count_share, values_fill = "0") %>%
        select(variable, level, everything()) %>% 
      left_join(total_summary %>% select(variable, level, total_count = count))
}


# Function to rename and reorder variables and levels
apply_var_and_level_names <- function(result_table, var_names, level_names) {

  # Order
  result_table <- result_table %>%
    arrange(match(variable, var_names$old),
            match(paste(variable, level), paste(level_names$var, level_names$level_old)))
  
    # Renaming levels
    result_table <- result_table %>%
        left_join(level_names, by = c("variable" = "var", "level" = "level_old")) %>%
        mutate(level = ifelse(is.na(level_new), level, level_new)) %>%
        select(-level_new)

        # Renaming variables
    result_table <- result_table %>%
        left_join(var_names, by = c("variable" = "old")) %>%
        mutate(variable = ifelse(is.na(new), variable, new)) %>%
        select(-new)

    
    result_table
}

result_table <- purrr::map_dfr(variables, ~summarize_cat_variable(dataset, .x))

# Apply renaming and reordering
final_table <- apply_var_and_level_names(result_table, var_names, level_names)

final_table %>% 
  gt(groupname_col = "variable", rowname_col = "level") %>%
  tab_stubhead(label = "Variable") %>%
  cols_label(total_count = "") %>% 
  tab_spanner("Diversity domain", Demographic:`Job-related`) %>% 
  sub_missing(columns = everything(), missing_text = "(missing)") %>% 
  gt_apa_style() %>% tab_style(
        style = list(cell_text(weight = "bold")),
        locations = cells_row_groups()
    ) %>% 
      tab_style(
        style = cell_text(align = "left", indent = px(15)),
        locations = cells_stub()
    ) %>% 
  tab_header("Distribution of effect sizes across moderator variables") %>% 
    gt_plt_bar(column = total_count, keep_column = FALSE, color = "grey")
  

  
```




# Main meta-analysis

```{r main, include=FALSE}
# Estimate covariance matrix for CHE meta-analysis, following Harrer:
# https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/multilevel-ma.html#fit-rve

V <- with(dataset, 
          impute_covariance_matrix(vi = var_adj,
                                   cluster = articlestudy,
                                   r = rho))

meta_model_intercept <- rma.mv(r_adj ~ 1 + domain,
                    V = V,
                    random = ~ 1 | articlestudy/effect_id,
                    data = dataset,
                    test="t",
                    dfs="contain",
                    sparse = TRUE)

meta_model <- rma.mv(r_adj ~ 0 + domain,
                    V = V,
                    random = ~ 1 | articlestudy/effect_id,
                    data = dataset,
                    test="t",
                    dfs="contain",
                    sparse = TRUE)


confidence_intervals <- conf_int(meta_model, vcov = "CR2", p_values = TRUE)

# Equivalence testing not implemented with RVE, but equivalent to inverting confidence intervals 
# (Campbell, 2023: https://arxiv.org/abs/2004.01757)
# For example, we will reject the above null hypothesis (H0: βk ≤ ∆k, lower or βk ≥ ∆k, upper), at a α significance level,
# whenever a (1-2α)% CI for βk fits entirely within (∆k, lower, ∆k, upper).
# To continue using RVE, we will do that here by searching for the smallest p-value for which this is true
# p-value for test can be max(p-lower, p-upper) - so for symetric CIs and bounds, only need to consider one direction

get_p_value <- function(model, coef, p_precision = .001, min_p = .0001, delta = .1) {
  
  confidence_interval <- conf_int(model, vcov = "CR2", coefs = coef, level = 1 - 2 * min_p)
  
  # Identify whether we care about lower or upper bound
  direction <- if (confidence_interval$beta < 0) "CI_L" else "CI_U"
  
  # Test boundary conditions
  # Estimate outside interval
  if (abs(confidence_interval$beta) >= delta) return(1)
  
  # p < min_p
  if (abs(confidence_interval[[direction]]) < delta) {
    message("Test significant at min_p, so returning 0. Make sure to report as <= ", format(min_p, scientific = FALSE))
    return(0)
  }
  
  # p >= .5
  confidence_interval <- conf_int(model, vcov = "CR2", coefs = coef, level = .001)
  if (abs(confidence_interval[[direction]]) > delta) {
    warning("Test not significant at *p* = .495. One-sided tests cannot identify two-sided p-value above .5, so returning NA")
    return(NA)
  }
  
  alpha_high <- .495
  alpha_low <- min_p
  
  while (alpha_high - alpha_low > p_precision) {
    alpha_mid <- (alpha_high + alpha_low) / 2
    confidence_interval <- conf_int(model, vcov = "CR2", coefs = coef, level = 1 - 2 * alpha_mid)
    
    if (abs(confidence_interval[[direction]]) < delta) {
            alpha_high <- alpha_mid
    } else {
      alpha_low <- alpha_mid
    }
    }

    return(alpha_high)
  
}

confidence_intervals <- confidence_intervals %>% rowwise() %>% 
  mutate(`equiv_.1` = get_p_value(meta_model, Coef, delta = .1),
         `equiv_.05` = get_p_value(meta_model, Coef, delta = .05)) %>% 
  ungroup()
```

## Report average effects and heterogeneity

```{r results='hide'}
{
meta_summary <- summary(meta_model) %>% capture.output()
meta_summary_intercept <- summary(meta_model_intercept)  %>% capture.output()
}

I2s <- mlm.variance.distribution(meta_model)

# Calculate credibility intervals, following James Pustejovsky
# https://stat.ethz.ch/pipermail/r-sig-meta-analysis/2019-April/001508.html

cred_upper <- confidence_intervals$beta + 1.282 * sqrt(sum(meta_model$sigma2) + confidence_intervals$SE^2)
cred_lower <- confidence_intervals$beta - 1.282 * sqrt(sum(meta_model$sigma2) + confidence_intervals$SE^2)
```


```{r}
confidence_intervals %>% 
  mutate(Domain = Coef %>% str_remove("domain") %>% str_replace("_", "-"),
         `*r*` = paste(fmt_cor(beta, 3), fmt_ci(CI_L, CI_U, 3), sigstars(p_val)),
         `|*r*| < .1` = fmt_p(`equiv_.1`),
         `|*r*| < .05` = fmt_p(`equiv_.05`),
         `Credibility\ninterval` = fmt_ci(cred_lower, cred_upper, 3)) %>% 
  select((ncol(.)-4):ncol(.)) %>% 
  gt() %>% 
  fmt_labels_md() %>% 
  tab_source_note(md(paste0("\\", timesaveR:::.make_stars_note(timesaveR:::std_stars[-1])))) %>% 
  gt_apa_style()
  
```

RQ1a: Is the link between diversity and team performance insubstantial (i.e., |r| < .1)? Does this differ between the dimensions of diversity?

Average effects significantly differed between diversity domains, `r meta_summary_intercept[str_detect(meta_summary_intercept, "^F\\(")]` (though not substantially so? TK - run equivalence test for this?)
After accounting for that, significant amount of heterogeneity remained, `r meta_summary[str_detect(meta_summary, "^QE")]`

`r fmt_pct(as.numeric(I2s$result$I2[3])/100)` of the total variance in effect sizes can be attributed to study-level heterogeneity (I<sup>2</sup><sub>Level 3</sub>), while `r fmt_pct(as.numeric(I2s$result$I2[2])/100)` can be attributed to heterogeneity between the effects studied (I<sup>2</sup><sub>Level 2</sub>).

The 80% credibility intervals were very wide (Table x), so that substantial positive and negative associations can be regularly expected. Therefore, moderation analyses were crucial.

## Distribution of effect sizes

```{r fig.height=12}

# Get confidence intervals based on the adjusted standard errors (for effective sample size and unreliability)
get_cis <- function(r_adj, se_adj, conf.level = 0.95) {
  
  # Reduce extreme estimates to those that would be rounded
  if (r_adj == 1) r_adj <- .995
  if (r_adj == -1) r_adj <- -.995

    # Fisher's r to z transformation
  z <- 0.5 * log((1 + r_adj) / (1 - r_adj))
  
  alpha <- 1 - conf.level
  z.critical <- qnorm(1 - alpha / 2)
  
  # Confidence interval calculation
  ci.lower <- z - z.critical * se_adj
  ci.upper <- z + z.critical * se_adj
  
  # Transforming back to r
  ci.lower.r <- (exp(2 * ci.lower) - 1) / (exp(2 * ci.lower) + 1)
  ci.upper.r <- (exp(2 * ci.upper) - 1) / (exp(2 * ci.upper) + 1)
  
  list(ci_low = ci.lower.r, ci_high = ci.upper.r)
}

dataset_spec <- dataset %>%
  mutate(ci = pmap(list(r_adj, se), get_cis)) %>%
  mutate(ci_low = map_dbl(ci, 1), ci_high = map_dbl(ci, 2)) %>% 
  select(-ci) %>% arrange(r_adj) %>% rowid_to_column(var = "specification") %>% 
  mutate(color = case_when(
    ci_high < 0 ~ "darkred",
    ci_low <= 0 ~ "darkgrey",
    ci_low > 0 ~ "darkgreen"
  ))

# Simplified function for creating the specification curve-type plot
# Derived from https://github.com/masurp/specr

  plot_a <- dataset_spec %>% 
    ggplot(aes(x = specification, y = r_adj, ymin = ci_low, ymax = ci_high)) +
    geom_pointrange(aes(color = color), alpha = 0.5, size = 0.6, fatten = 1) +
    theme_minimal() +
    scale_color_identity() +
    theme(axis.line = element_line(size = 0.5), legend.position = "none",
          panel.spacing = unit(0.75, "lines"), axis.text = element_text(color = "black")) +
    labs(x = "", y = "r (adjusted)") +
  geom_hline(yintercept = 0)

  choices <- c("Domain" = "domain", "Top team" = "tmt", "Students" = "stud_sample",
               "Interdependence" = "interdep", "Complexity" = "complexity", "Longevity" = "longevity",  
               "Measure" = "meas_type", "Rater" = "rater", "Hypothesis" = "art_focus")
  
  rev_choices <- setNames(names(choices), choices)
  
  levels_replace <- level_names %>% select(-var) %>% distinct() %>% 
    {setNames(.$level_new, .$level_old)}
  
 levels_replace <- tibble::tribble(
  ~level_old,                 ~level_new,                 
   "focal H",                  "Focal hypothesis",        
   "auxiliary H",              "Auxiliary hypothesis",    
   "descriptive",              "Descriptive",             
   "high",                     "High",                    
   "medium",                   "Medium",                  
   "low",                      "Low",                     
   "yes",                      "Yes",                     
   "no",                       "No",                      
   "Variety",                  "Variety",                 
   "Separation",               "Separation",              
   "Other",                    "Other",                   
   "Objective",                "Objective",               
   "Subjective - self",        "Subjective - Self",       
   "Subjective - supervisor",  "Subjective - Supervisor", 
   "Subjective - external",    "Subjective - External",   
   "hours",                    "Hours",                   
   "days",                     "Days",                    
   "weeks",                    "Weeks",                   
   "months",                   "Months",                  
   "years",                    "Years",                   
   "stable",                   "Stable",
   "Demographic",              "Demographic", 
   "Cognitive",                "Cognitive",   
   "Job-related",              "Job-Related"
) %>% {
  setNames(.$level_new, .$level_old)
}
  
  plot_b <- dataset_spec %>%
    mutate(sub_dom = fct_lump_n(sub_dom, 5)) %>% 
    tidyr::gather(key = "key", value = "value", choices) %>%
    mutate(key = coalesce(rev_choices[key], key),
           value = coalesce(levels_replace[value], value),
           key = factor(key, levels = rev_choices),
           value = factor(value, levels = levels_replace) %>% fct_rev()) %>% 
    filter(!is.na(value)) %>% 
ggplot(aes(x = specification, y = value, color = color)) +
    geom_point(shape = 124, size = 3.35, alpha = .5) +
    scale_color_identity() +
    theme_minimal() +
    facet_grid(key ~ ., scales = "free_y", space = "free_y", switch = "y") +
    theme(axis.line = element_line(size = 0.5), 
          legend.position = "none",
          panel.spacing = unit(0.75, "lines"), 
          axis.text = element_text(color = "black"),
          strip.text.y.left = element_text(angle = 90, face="bold"),
          strip.background = element_rect(fill="lightblue", colour="black",size=1),
          strip.placement = "outside",
          strip.text.x = element_blank()) +
    labs(x = "Effect sizes (ranked)", y = "")


  # Combine both plots
  p <- plot_a / plot_b +
    plot_layout(heights = c(1, 4))

  p
```




# Meta-regression


## Impute data

- Exclude variables with > 95% missing (psych safety and diversity climate) - no variance based on criteria either
- How to deal with MNAR variables (virtuality and authority diff)? Would require many post-hoc choices - so in spirit of RR, omit?

```{r}

mi_vars <- c("art_focus", "pub_status", "design", "setting", "ind_sector", "team_function", "country", "n_teams", "stud_sample", "tmt", "domain", "sub_dom", "meas_type", "criterion", "rater", "interdep", "complexity", "longevity", "power_distance", "collectivism", "year_merged", "citation_count")

convert_to_factor <- function(x) {
  if (is.character(x)) {
    return(as.factor(x))
  } else {
    return(x)
  }
}

dataset <- dataset %>% 
  mutate(across(any_of(mi_vars), convert_to_factor))

# Specify ordered factors
dataset$longevity <- factor(dataset$longevity, levels = c("hours", "days", "weeks", "months", "years", "stable"), ordered = TRUE)
dataset$interdep <- factor(dataset$interdep, levels = c("low", "medium", "high"), ordered = TRUE)
dataset$complexity <- factor(dataset$complexity, levels = c("low", "medium", "high"), ordered = TRUE)

# Get matrices
imp <- mice(dataset, maxit=0, seed = 1526)


# Set variables of interest to be predicted by each other, all others ignored
preds <- imp$predictorMatrix
preds[] <- 0  
# pred_matrix <- dataset %>% select(all_of(mi_vars)) %>% quickpred_ext() <- could reduce the matrix like this
# preds[rownames(pred_matrix), colnames(pred_matrix)] <- pred_matrix
preds[mi_vars, mi_vars] <- 1

# Use other model variables in MI model
preds[mi_vars, "r_adj"] <- 1
preds[mi_vars, "se"] <- 1



# Standard methods per variable type are generally good (polyreg for factors, polr for ordered, pmm for continuous)  
# but polyreg does not work well with many factor levels - so these set to pmm
methods <- imp$method
methods[c("ind_sector", "team_function", "country", "sub_dom")] <- "pmm"
methods[!names(methods) %in% mi_vars] <- ""

# Van Buren - set m conservatively to share of incomplete cases
m <- {dataset %>% select(all_of(mi_vars)) %>% naniar::prop_miss_case() * 100} %>% ceiling()
x

domains <- dataset$domain
# Ensure that sub-domains are only imputed within each domain (otherwise get invalid imputations)
mice.impute.imp_sub_domain <- function(y, ry, x, ...) {
  # y is the vector to be imputed
  # ry is a logical vector indicating which values in y are missing
  # x is the matrix of predictor variables, already filtered by predMatrix
  dots <- list(...)

  # Domain information cannot be extracted from x as it is dummy-coded there, so take from global env
  unique_domains <- unique(domains)  # Consider only domains where y is missing
  
  imputed <- rep(NA, sum(dots$wy))  # Start with y and replace missing values

  imps <- list()
  
  for (d in unique_domains) {
    # Subset data for the current domain
    rows_in_domain <- which(domains == d)  # Rows in this domain where y is missing
    y_sub <- y[rows_in_domain]
    x_sub <- x[rows_in_domain, ]
    ry_sub <- ry[rows_in_domain]
    dots_sub <- dots
    dots_sub$wy <- dots_sub$wy[rows_in_domain]
    
    # Apply polyreg imputation method for the current domain
    args_list <- c(list(y_sub, ry_sub, x_sub), dots_sub)
    imp <- do.call("mice.impute.polyreg", args_list)
    
    # Assign the imputed values back to the result vector
    #imputed[rows_in_domain] <- imp
    
    imps[d] <- list(imp)
  }
  
    # Order imps content based on domains and locations specified in wy
  ds <- domains[dots$wy]
  
    for (d in unique_domains) {
      imputed[ds == d] <- unlist(imps[d])
    }    
  
  return(imputed)
}

methods["sub_dom"] <- "imp_sub_domain"

imp <- mice(dataset, m = m, seed = 1526, predictorMatrix = preds, method = methods)
```

## Individually test moderators

Partly for consistency with earlier work, partly as we cannot determine significance of moderators (rather than individual levels) from meta-regression as we cannot presently pool Q-test (or Wald test) results (https://www.jepusto.com/mi-with-clubsandwich/)

RQ 1: Does team diversity predict team performance? How does this differ between the dimensions of diversity and the performance task under consideration?

RQ1a: Is the link between diversity and team performance insubstantial (i.e., |r| < .1)? Does this differ between the dimensions of diversity?

H1: Diversity has a substantial positive association with performance when the task is high in complexity.
H2: Diversity has a more positive association with team performance when the task requires a high level of interdependence.

H3a: Diversity has a more negative link to performance in tasks that focus on maximizing production of an output with a pre-defined strategy. (DON'T HAVE DATA?)
H3b: Diversity has a more positive link to performance in tasks where performance depends on creative divergence rather than convergence. (NEED TO DO WITH OBSERVED DATA - NO IMPUTATION)

RQ2: How does the relationship between diversity and performance differ across space and time?
RQ2a: Is the relationship between team diversity and performance related to a country’s level of collectivism versus individualism?

H4a: The relationship between diversity (particularly demographic diversity) and team performance has become more positive over time.
H4b: The relationship between diversity and team performance is positive and substantial (i.e. r > .1) in evidence from the past decade (2012-2022).

RQ3: How do contextual factors influence the relationship of diversity with team performance?
H5: Diversity has a more positive link to performance when the team works in a context that has a positive diversity climate. (DON'T HAVE DATA?)
H6: Diversity has a more positive link to performance when teams experience high levels of psychological safety. (DON'T HAVE DATA?)
H7: Diversity has a more positive link to performance when the team is low in authority differentiation than when it is high in authority differentiation. (REPLACE BY POWER DISTANCE?)

RQ3a: How does the link between diversity and performance differ depending on teams’ level of virtuality? (PROBLEMATIC DATA)

RQ3b: How does the link between diversity and performance differ depending on the longevity of a team?

RQ4: How do methodological choices influence the relationship of diversity with team performance?
RQ4a: How does the link between diversity and performance differ depending on whether performance is rated subjectively or measured objectively?
H8: Diversity will have more positive associations with performance where it is measured as variety rather than separation.
H9: Studies where the link between diversity and performance is the focal hypothesis will report larger (H8a) and more positive (H8b) effect sizes than studies where this is an auxiliary or descriptive result.

- Compare published vs unpublished ES
- Design (exp + quasi vs Observational)

Citations:
- Correlated with ES? [Cannot meaningfully assess signifiance due to prevalent repeated measures]

### With observed data

```{r}
# See https://www.jepusto.com/rve-meta-analysis-with-mi/#

cat_moderators <- c("complexity", "interdep", "longevity", "meas_type", "rater", "design", "art_focus")
cont_moderator <- c("year_merged", "collectivism", "power_distance")

single_cat_moderator_tests <- cat_moderators %>% set_names() %>% map(\(moderator) {
  message("Now estimating ", moderator)
  
  no_levels <- dataset[[moderator]] %>%
    unique() %>%
    na.omit() %>%
    length()
  
  V <- with(dataset, 
          impute_covariance_matrix(vi = var_adj,
                                   cluster = articlestudy,
                                   r = rho))
  
  mod <- {
    V <- V # Assuming var_adj is the observed covariance matrix
    dataset[[moderator]] <- factor(dataset[[moderator]], ordered = FALSE)

    mod <- rma.mv(as.formula(paste0("r_adj ~ ", moderator)),
      V = V,
      random = ~ 1 | articlestudy / effect_id,
      test = "t",
      dfs = "contain",
      data = dataset,
      sparse = TRUE
    )

    mod_domains <- rma.mv(as.formula(paste0("r_adj ~ 0 + domain + domain:", moderator)),
      V = V,
      random = ~ 1 | articlestudy / effect_id,
      test = "t",
      dfs = "contain",
      data = dataset,
      sparse = TRUE
    )

    list(
      mod = mod,
      mod_domains = mod_domains
    )
  }
  
  robust_across <- coef_test(mod$mod, vcov = "CR2")
  robust_domain <- coef_test(mod$mod_domains, vcov = "CR2")

  # Tests for each moderator and domain
  

  # Test of moderator overall
  res_tibble <- Wald_test(mod$mod, vcov = "CR2", constrain_zero(2:nrow(robust_across), coef(mod$mod))) %>% mutate(domain = "Overall")

  # Test of domains

  for (i in 1:3) {
      # Test of moderator overall
    res_tibble <- bind_rows(res_tibble, Wald_test(mod$mod_domains, vcov = "CR2", constrain_equal(seq(i, no_levels * 3, by = 3), coef(mod$mod_domains))) %>% 
        mutate(domain = names(coef(mod$mod_domains))[i]))
  }
  
  list(moderator_test = res_tibble, robu_averages = robust_across, robu_domains = robust_domain, meta_models = mod)
})


single_cont_moderator_tests <- cont_moderator %>% set_names() %>% map(\(moderator) {
  message("Now estimating ", moderator)
  
 
  V <- with(dataset, 
          impute_covariance_matrix(vi = var_adj,
                                   cluster = articlestudy,
                                   r = rho))
  
  mod <- {
    V <- V # Assuming var_adj is the observed covariance matrix

    mod <- rma.mv(as.formula(paste0("r_adj ~ ", moderator)),
      V = V,
      random = ~ 1 | articlestudy / effect_id,
      test = "t",
      dfs = "contain",
      data = dataset,
      sparse = TRUE
    )

    mod_domains <- rma.mv(as.formula(paste0("r_adj ~ 0 + domain + domain:", moderator)),
      V = V,
      random = ~ 1 | articlestudy / effect_id,
      test = "t",
      dfs = "contain",
      data = dataset,
      sparse = TRUE
    )

    list(
      mod = mod,
      mod_domains = mod_domains
    )
  }
  
  list(robust_across = coef_test(mod$mod, vcov = "CR2"),
  robust_domain = coef_test(mod$mod_domains, vcov = "CR2"),
  meta_models = mod)
})

ks <- single_cat_moderator_tests %>% map_dbl(~.x$meta_models$mod_domains$k) %>% tibble(moderator = names(single_cat_moderator_tests), k = .)

ks <- bind_rows(ks, 
                single_cont_moderator_tests %>% map_dbl(~.x$meta_models$mod_domains$k) %>% tibble(moderator = names(single_cont_moderator_tests), k = .))

ks <- ks %>% rowwise() %>% mutate(moderator = var_names$new[var_names$old == moderator])

cat_summary <- single_cat_moderator_tests %>% map_dfr("moderator_test", .id = "moderator") %>%  mutate(domain = str_remove(domain, "domain"))

cont_summary <- bind_rows(
  single_cont_moderator_tests %>% map_dfr("robust_across", .id = "moderator") %>% as_tibble() %>%  mutate(domain = "Overall") %>% filter(Coef != "intrcpt"),
  single_cont_moderator_tests %>% map_dfr("robust_domain", .id = "moderator") %>% as_tibble() %>%  filter(str_detect(Coef, ":")) %>% 
    mutate(domain = str_extract(Coef, ".*:") %>% str_remove("domain") %>% str_remove(":"))
)



bind_rows(cat_summary %>% select(moderator, p_val, domain),
          cont_summary %>% select(moderator, p_val = p_Satt, domain)) %>% 
  rowwise() %>% 
  mutate(p_fmt = paste(fmt_p(p_val, equal_sign = FALSE), sigstars(p_val)),
         moderator = var_names$new[var_names$old == moderator]) %>%
    select(-p_val) %>% 
  spread(domain, p_fmt) %>% 
  left_join(ks) %>% 
  select(Moderator = moderator, k, Overall, Demographic, Cognitive, `Job-related`) %>% gt::gt() %>% 
  tab_header(title = "Univariate moderator tests") %>% 
  tab_spanner(md("Significance tests (*p*-values)"), Overall:`Job-related`) %>% 
  gt::fmt(fns = md) %>% 
  gt::tab_source_note(timesaveR:::std_stars %>% {glue::glue_collapse(paste(names(.),.), ", ")} %>% html())
```


```{r}
# From https://www.jepusto.com/mi-with-clubsandwich/
pool_robust <- function(models) {
  models %>% 
  map(coef_test, vcov = "CR2") %>% 
    # add coefficient names as a column
    lapply(function(x) {
      x$coef <- row.names(x)
      x
    }) %>%
    bind_rows() %>%
    as.data.frame() %>%
    # summarize by coefficient
    group_by(coef) %>%
    summarise(
      m = n(),
      B = var(beta),
      beta_bar = mean(beta),
      V_bar = mean(SE^2),
      eta_bar = mean(df_Satt)
    ) %>%
    mutate(
      # calculate intermediate quantities to get df
      V_total = V_bar + B * (m + 1) / m,
      gamma = ((m + 1) / m) * B / V_total,
      df_m = (m - 1) / gamma^2,
      df_obs = eta_bar * (eta_bar + 1) * (1 - gamma) / (eta_bar + 3),
      df = 1 / (1 / df_m + 1 / df_obs),

      # calculate summary quantities for output
      se = sqrt(V_total),
      t = beta_bar / se,
      p_val = 2 * pt(abs(t), df = df, lower.tail = FALSE),
      crit = qt(0.975, df = df),
      lo95 = beta_bar - se * crit,
      hi95 = beta_bar + se * crit
    ) %>%
    select(coef, est = beta_bar, se, t, df, p_val, lo95, hi95, gamma)
}

pooled_Wald <- function(models, which_coefs) {
# Get coefficients
  coefs <- models %>% map(coef)

  # Get CR2 variance-covariance matrices
  vcovs <- models %>% map(vcovCR, type = "CR2")
  
   ref_coefs <- coefs[[1]] %>% names() %>% str_detect(., which_coefs) %>% which()
 
  
   message("For ", which_coefs, " now testing: ", coefs[[1]] %>% names() %>% .[ref_coefs] %>% paste(collapse = " "))
   
  # Get denominator df from complete-data F-tests
  q <- length(ref_coefs)

  eta <- numeric()

    dfs <-
      models %>%
      map_dfr(Wald_test, vcov = "CR2", constraints = constrain_zero(ref_coefs), test = "HTZ") %>%
      mutate(
        eta = df_denom + (q - 1)
      )
    eta <- mean(dfs$eta)
  


  # Combine imputed results
  # These estimates appear to be the same as the more complicated approach - test with larger sample, then simplify
  res <- MIcombine(results = coefs, variances = vcovs)
  res$coefficients

    Cmat <- constrain_zero(ref_coefs, coefs = res$coefficients)
    Q <- t(Cmat %*% res$coefficients) %*% solve(Cmat %*% res$variance %*% t(Cmat)) %*% (Cmat %*% res$coefficients)
    Fstat <- (eta  - q + 1) / (eta  * q) * as.numeric(Q)
    p_val <- pf(Fstat, df1 = q, df2 = eta - q + 1, lower.tail = FALSE)
   tibble(query = which_coefs, Fstat = Fstat, df_num = q, df_denom = eta - q + 1, p_val = p_val)
}

```

## Run meta-regression

Citation count ommitted as it was registered as exploratory and logically cannot predict (precede) effect size

```{r}


mod <- with(imp, {V <-  impute_covariance_matrix(vi = var_adj,
                                   cluster = articlestudy,
                                   r = rho)
  
  complexity <- factor(complexity, ordered = FALSE) %>% relevel(ref = "high")
  interdep <- factor(interdep, ordered = FALSE) %>% relevel(ref = "high")
  longevity <- factor(longevity, ordered = FALSE) %>% relevel(ref = "stable")
  art_focus <- factor(art_focus) %>% relevel(ref = "focal H")
  meas_type <- factor(meas_type) %>% relevel(ref = "Variety")
  rater <- factor(rater) %>% relevel(ref = "Objective")
  design <- factor(design) %>% relevel(ref = "Observational")

  mod <- rma.mv(r_adj ~ year_merged + complexity + interdep + collectivism + 
                                 longevity + meas_type + rater + design + art_focus,
                              V = V,
                              random = ~ 1 | articlestudy/effect_id,
                              test = "t",
                              dfs = "contain",
                              sparse = TRUE)

  mod_interact <- rma.mv(r_adj ~ domain * year_merged + domain * complexity + domain * interdep + domain * collectivism + 
                                 domain * longevity + domain * meas_type + domain * rater + domain * design + domain * art_focus,
                              V = V,
                              random = ~ 1 | articlestudy/effect_id,
                              test = "t",
                              dfs = "contain",
                              sparse = TRUE)
  
  list(mod = mod, mod_interact = mod_interact)
  
})

coefs_across <- map(mod$analyses, "mod") %>% pool_robust()
coefs_domains <- map(mod$analyses, "mod_interact") %>% pool_robust()

cat_moderators <- c("complexity", "interdep", "longevity", "meas_type", "rater", "design", "art_focus")
cont_moderator <- c("year_merged", "collectivism", "power_distance")

cat_moderators_regex <- c(paste0("^", cat_moderators), 
                          paste0("Cognitive:", cat_moderators), 
                          paste0("Job-related:", cat_moderators))

# Loop over domains (^, Cognitive:, Job-related:), merge all results
cont_interaction_tests <- map(mod$analyses, "mod_interact") %>% {
  map_dfr(cat_moderators_regex, \(query) pooled_Wald(., query))
} %>% 
  mutate(domain = case_when(str_detect(query, "^\\^") ~ "Demographic",
                            str_detect(query, "Cognitive:") ~ "Cognitive",
                            str_detect(query, "Job-related:") ~ "Job-related"),
         moderator = query %>% str_remove("^\\^") %>% 
            str_remove("Cognitive:") %>% 
           str_remove("Job-related:"),
         moderator = sapply(moderator, function(coeff) {
    matches <- str_detect(coeff, paste0("^", cat_moderators))
    if(any(matches)) {
      return(cat_moderators[which.max(matches)]) # Returns the first matched moderator
    } else {
      warning("Failed moderator match: ", coeff)
      return(NA)
    }
  }))

# Loop over average results:
cont_avg_tests <- map(mod$analyses, "mod") %>% {
  map_dfr(cat_moderators, \(query) pooled_Wald(., query))
} %>% 
  mutate(domain = "Overall", moderator = sapply(query, function(coeff) {
    matches <- str_detect(coeff, paste0("^", cat_moderators))
    if(any(matches)) {
      return(cat_moderators[which.max(matches)]) # Returns the first matched moderator
    } else {
      return(NA)
    }
  }))

# Combine all results into 1 table
coefs_domains %>% filter(map(cont_moderator, \(m) str_detect(coef, m)) %>% Reduce(`|`, .)) %>% 
  mutate(domain = case_when(str_detect(coef, "Cognitive:") ~ "Cognitive",
                            str_detect(coef, "Job-related:") ~ "Job-related",
                            .default = "Demographic"),
         moderator = coef %>% str_remove(".*:")) %>% 
  bind_rows(coefs_across %>% filter(coef %in% cont_moderator) %>% transmute(domain = "Overall", moderator = coef, p_val)) %>% 
  bind_rows(cont_interaction_tests, cont_avg_tests) %>% 
 select(domain, moderator, p_val) %>% 
rowwise() %>% 
  mutate(         p_fmt = paste(fmt_p(p_val, equal_sign = FALSE), sigstars(p_val)),
         moderator = var_names$new[var_names$old == moderator]) %>%
    select(moderator, domain, p_fmt) %>% 
  spread(domain, p_fmt) %>% 
  select(Moderator = moderator, Overall, Demographic, Cognitive, `Job-related`) %>% gt::gt() %>% 
  tab_header(title = "Multivariate (meta-regression) moderator tests") %>% 
  tab_spanner(md("Significance tests (*p*-values)"), Overall:`Job-related`) %>% 
  gt::fmt(fns = md) %>% 
  gt::tab_source_note(timesaveR:::std_stars %>% {glue::glue_collapse(paste(names(.),.), ", ")} %>% html())

# TK: Deep-dive into significant ones & all in appendix

```

# Meta-CART

## Combine ES

```{r}
dataset_aggregate <- dataset %>% 
  group_by(articlestudy, pub_status, art_focus, year_merged, design, rater, collectivism, interdep, 
           complexity, longevity, virtuality, domain, sub_dom, meas_type) %>% 
  mutate(es_aggregate_id = cur_group_id()) %>% 
  ungroup()
  
dataset_aggregate <- escalc(yi = r_adj, vi = var_adj, data = dataset_aggregate) %>% 
  aggregate.escalc(cluster = es_aggregate_id, rho = rho, struct = "CS", na.rm = FALSE)

# Remove escalc class to avoid future issues
dataset_aggregate <- as_tibble(dataset_aggregate)

set.seed(2124)
datasets_sampled <- dataset_aggregate %>% 
  group_by(articlestudy, domain) %>% 
  slice_sample(n = 1) %>% 
  ungroup() 
```

## Impute best case

```{r best-imputation-functions}

dummy_code_df <- function(df, exclude_column, drop_first = TRUE) {
  if (!is.character(exclude_column)) {
    stop("exclude_column must be a character string.")
  }
  
  for (col_name in names(df)) {
    if (col_name != exclude_column && is.factor(df[[col_name]]) && !is.ordered(df[[col_name]])) {

      # Get levels and potentially drop the first one
      levels_to_dummy <- levels(df[[col_name]])
      if (drop_first) {
        levels_to_dummy <- levels_to_dummy[-1]
      }

      # Create dummy columns for each level
      for (level in levels_to_dummy) {
        dummy_name <- paste0(col_name, "_", level) %>% vctrs::vec_as_names(repair = "universal_quiet")
        df[[dummy_name]] <- df[[col_name]] == level
      }
      
      # Remove the original factor column if not excluded
      df[[col_name]] <- NULL
    }
  }
  
  return(df)
}

# Imputation code inspired from mice package, copyright Stef van Buuren - thanks

impute_best_polr <- function(dat, var_name) {
  
  dat <- dummy_code_df(dat, var_name)
  
  collinear <- mice:::find.collinear(dat)
  
  if (length(collinear) > 0) {
  
  dat <- dat[-which(names(dat) %in% setdiff(collinear, var_name))]
  }
  
  y_missing <- which(is.na(dat[var_name]))
  
  f <- setdiff(colnames(dat), var_name) %>% glue::glue_collapse(sep = " + ") %>% paste(var_name, "~", .)
  
  complete_dat <- dat %>% drop_na(everything())
  
  ## polr may fail on sparse data. We revert to multinom in such cases.
  fit <- try({
    MASS::polr(as.formula(f),
      data = complete_dat,
    )
      post <- predict(fit, dat[is.na(dat[var_name]), ], type = "probs")
  })
  if (inherits(fit, "try-error")) {
      message("polr falls back to multinom")
    fit <- nnet::multinom(as.formula(f),
      data = complete_dat,
      maxit = 100, trace = FALSE,
      MaxNWts = 1500
    )
      post <- predict(fit, dat[is.na(dat[var_name]), ], type = "probs")
  }
  
  
  if (length(y_missing) == 1) {
    temp <- matrix(post, nrow = 1, ncol = length(post))
    colnames(temp) <- names(post)
    post <- temp
  }

max_col_index <- apply(post, 1, \(x) {
  if(all(is.na(x))) {
    return(NA)
  } else {
    return(which(x == max(x, na.rm = TRUE)))
  }
})

new_y <- colnames(post)[max_col_index]


  if(length( dat[[var_name]][y_missing] ) != length(new_y)) browser()
  
  dat[[var_name]][y_missing] <- new_y
  
    message("Imputed ", sum(!is.na(dat[[var_name]][y_missing])), " values.")
  
    
  dat[[var_name]]
}

impute_best_polyreg <- function(dat, var_name) {
  
  dat <- dummy_code_df(dat, var_name)
  
  collinear <- mice:::find.collinear(dat)
  
  if (length(collinear) > 0) {
  
    dat <- dat[-which(names(dat) %in% setdiff(collinear, var_name))]
  }
  
  y_missing <- which(is.na(dat[var_name]))
  
  f <- setdiff(colnames(dat), var_name) %>% glue::glue_collapse(sep = " + ") %>% paste(var_name, "~", .)
  
  complete_dat <- dat %>% drop_na(everything())
  
  
    fit <- nnet::multinom(as.formula(f),
      data = complete_dat,
      maxit = 100, trace = FALSE,
      MaxNWts = 3000
    )
    
  tryCatch({
    post <- predict(fit, dat[is.na(dat[var_name]), ], type = "probs")
  if (length(y_missing) == 1) {
    temp <- matrix(post, nrow = 1, ncol = length(post))
    colnames(temp) <- names(post)
    post <- temp
  }

max_col_index <- apply(post, 1, \(x) {
  if(all(is.na(x))) {
    return(NA)
  } else {
    return(which(x == max(x, na.rm = TRUE)))
  }
})
new_y <- colnames(post)[max_col_index]

  
  dat[[var_name]][y_missing] <- new_y
    },
  error = \(e) {
    warning("Prediction failed")
    return(dat[[var_name]])
    })

  
  message("Imputed ", sum(!is.na(dat[[var_name]][y_missing])), " values.")  
    
  dat[[var_name]]
}


```


```{r}
# Specify ordered factors
datasets_sampled <- datasets_sampled %>% mutate(across(c(where(is.character), -articlestudy), factor))

datasets_sampled$longevity <- factor(datasets_sampled$longevity, levels = c("hours", "days", "weeks", "months", "years", "stable"), ordered = TRUE)
datasets_sampled$interdep <- factor(datasets_sampled$interdep, levels = c("low", "medium", "high"), ordered = TRUE)
datasets_sampled$complexity <- factor(datasets_sampled$complexity, levels = c("low", "medium", "high"), ordered = TRUE)



# specify variables to include in hot-decking (those in metaCart and relevant other preds)
mi_vars <- c("art_focus", "pub_status", "design", "setting", "ind_sector", "team_function", 
             "n_teams", "stud_sample", 
             "tmt", "domain", "sub_dom", "meas_type", "rater", "interdep", "complexity", "longevity", "power_distance",
             "collectivism", "year_merged", "citation_count")

datasets_sampled_reduced <- datasets_sampled %>% select(articlestudy, domain, r_adj, se, var_adj, all_of(mi_vars))

### Impute BEST version

datasets_sampled_reduced_best <- datasets_sampled_reduced

missing_patterns <- function(dataset, target_var) {
  dataset <- dataset[is.na(dataset[[target_var]]), ]
  dataset %>% select(-matches(target_var)) %>%  
    mutate(missing_vars = pmap_chr(., function(...) {
    vars <- names(c(...))
    missing <- vars[is.na(c(...))]
    if (length(missing) == 0) return(NA_character_) # Return NA if no missing vars
    paste(missing, collapse = "|")
  })) %>% mutate(across(-missing_vars, ~ as.numeric(is.na(.)))) %>%
  mutate(m = rowSums(across(-missing_vars), na.rm = TRUE)) %>%
  group_by(missing_vars, m) %>%
  summarise(n = n(), .groups = 'drop') %>%
  arrange(m, -n)
}


impute_sequentially <- function(dataset, target_var, fun) {
  missings <- missing_patterns(dataset, target_var)
  if (any(is.na(missings$missing_vars)))  {
    dataset[[target_var]] <- do.call(fun, list(dataset, target_var))
  } 
  missings <- missings %>% filter(!is.na(missing_vars))
  if (nrow(missings) == 0) return(dataset[[target_var]])
  for (i in 1:nrow(missings)) {
    dataset[[target_var]] <- do.call(fun, list(dataset %>% select(-matches(missings[i,]$missing_vars)), target_var))
  }
  dataset[[target_var]]
}

polr_vars <- c("interdep", "complexity", "longevity")

polyreg_vars <- c("art_focus", "pub_status", "design", "setting", "ind_sector", "team_function", 
             "stud_sample", "tmt", "domain", "meas_type", "rater")

special_var <- c( "sub_dom")
             
pmm_vars <- c("power_distance", "collectivism")

for (var in polr_vars) {
  if (any(is.na(datasets_sampled_reduced_best[[var]]))) {
    message("\n\n Imputing ", var)
    datasets_sampled_reduced_best[[var]] <- impute_sequentially(datasets_sampled_reduced_best %>% select(-articlestudy), var, impute_best_polr)
  }
}

for (var in polyreg_vars) {
  if (any(is.na(datasets_sampled_reduced_best[[var]]))) {
        message("\n\n Imputing ", var)
    datasets_sampled_reduced_best[[var]] <- impute_sequentially(datasets_sampled_reduced_best %>% select(-articlestudy), var, impute_best_polyreg)
  }
}

# Impute sub-domains by domain, to ensure valid values
datasets_sampled_reduced_best_split <- split(datasets_sampled_reduced_best, datasets_sampled_reduced_best$domain)
for (domain in names(datasets_sampled_reduced_best_split)) {
  datasets_sampled_reduced_best_split[[domain]]$sub_dom <- impute_sequentially(datasets_sampled_reduced_best_split[[domain]] %>% select(-articlestudy), "sub_dom", impute_best_polyreg)
}

datasets_sampled_reduced_best <- bind_rows(datasets_sampled_reduced_best_split)


# For continuous variables, use PMM function in mice
imp <- mice(datasets_sampled_reduced_best, maxit = 0)

methods <- imp$method
methods[] <- ""
methods[pmm_vars] <- "pmm"
preds <- imp$predictorMatrix
preds[] <- 0  

preds[pmm_vars, mi_vars] <- 1
preds[pmm_vars, "r_adj"] <- 1
preds[pmm_vars, "se"] <- 1
preds[pmm_vars, pmm_vars] <- 0 # Are generally missing together, so this would not help

imp <- mice(datasets_sampled_reduced_best, m = 1, seed = 1600, predictorMatrix = preds, 
            method = methods, donors = 1)


datasets_sampled_reduced_best <- datasets_sampled_reduced_best %>% select(-all_of(pmm_vars)) %>% 
  left_join(imp %>% complete() %>% select(articlestudy, domain, all_of(pmm_vars)))


datasets_sampled_reduced_best <- datasets_sampled_reduced_best %>% left_join(
  datasets_sampled %>% select(articlestudy, domain, country)
)


### Impute random (worst case)

impute_with_random_draw <- function(dataframe) {
  for (col in names(dataframe)) {
    missing_indices <- which(is.na(dataframe[[col]]))
    if (length(missing_indices) > 0) {
      non_missing_values <- dataframe[[col]][!is.na(dataframe[[col]])]
      if (length(non_missing_values) > 0) {
        dataframe[[col]][missing_indices] <- sample(non_missing_values, length(missing_indices), replace = TRUE)
      }
    }
  }
  return(dataframe)
}

datasets_sampled_reduced_random <- datasets_sampled_reduced

datasets_sampled_reduced_random <- datasets_sampled_reduced_random %>% select(-sub_dom) %>% impute_with_random_draw() %>% 
  left_join(datasets_sampled_reduced_random %>% select(articlestudy, domain, sub_dom))

# Impute sub-domains by domain, to ensure valid values
datasets_sampled_reduced_random_split <- split(datasets_sampled_reduced_random, datasets_sampled_reduced_random$domain)
for (domain in names(datasets_sampled_reduced_random_split)) {
  datasets_sampled_reduced_random_split[[domain]] <- impute_with_random_draw(datasets_sampled_reduced_random_split[[domain]])
}

datasets_sampled_reduced_random <- bind_rows(datasets_sampled_reduced_random_split)
```

## Run meta-CART

Despite the description of the method as not requiring variable selection, model convergence depends on the variables included during estimation. We will therefore fit all possible models and aggregate results later.

```{r}
datasets_sampled_reduced_best$art_focus <- datasets_sampled_reduced_best$art_focus %>% relevel(ref = "descriptive")

mods <- c("domain", "year_merged", "complexity", "interdep", "collectivism", "longevity", "meas_type", "rater", "design", "art_focus")

all_subsets <- lapply(10:1, function(size) {
  combn(mods, size, simplify = FALSE)
})

if (file.exist("cached_results/cart_trees_best_imp.RDS")) {
  trees <- read_rds("cached_results/cart_trees_best_imp.RDS")
} else {
  trees <- list()

  for (cmb in all_subsets) {
    for (cur_mods in cmb) {
      # Provide intermittent progress update
      if (length(trees) %% 50 == 0) message("Fitting tree ", length(trees))
      preds <- paste0(cur_mods, collapse = " + ")

      if (!is.null(trees[[preds %>% str_replace_all(" \\+ ", "__")]])) next

      trees[preds %>% str_replace_all(" \\+ ", "__")] <- list(REmrt(as.formula(paste0("r_adj ~ ", preds)),
        datasets_sampled_reduced_best,
        vi = var_adj,
        c = 0,
        maxL = 20
      ))
    }
  }

  write_rds(trees, "cached_results/cart_trees_best_imp.RDS")
}

if (file.exists("cached_results/cart_trees_ahead_best_imp.RDS")) {
  trees_ahead <- read_rds("cached_results/cart_trees_ahead_best_imp.RDS")
} else {
  trees_ahead <- list()

  for (cmb in all_subsets) {
    for (cur_mods in cmb) {
      # Provide intermittent progress update
      if (length(trees_ahead) %% 50 == 0) message("Fitting lookahead tree ", length(trees_ahead))
      preds <- paste0(cur_mods, collapse = " + ")

      if (!is.null(trees_ahead[[preds %>% str_replace_all(" \\+ ", "__")]])) next

      # In special cases (with single predictor), lookahead = TRUE can fail - 
      # in these cases, we will not store the result. Bug was reported to package maintainers.
      trees_ahead[preds %>% str_replace_all(" \\+ ", "__")] <- list(tryCatch(REmrt(as.formula(paste0("r_adj ~ ", preds)),
        datasets_sampled_reduced_best,
        vi = var_adj,
        c = 0,
        maxL = 20,
        lookahead = TRUE
      ), error = function(e) NULL))
    }
  }

  write_rds(trees_ahead, "cached_results/cart_trees_ahead_best_imp.RDS")
}

```



## Run metaforest on full single-imputed datasets

```{r}
install.packages("metaforest")
library(metaforest)

datasets_sampled_reduced_best <- datasets_sampled_reduced_best %>% 
  group_by(articlestudy) %>% 
  mutate(study_id = cur_group_id()) %>% 
  ungroup()

set.seed(1705)
check_conv <- MetaForest(r_adj ~ domain + year_merged + complexity + interdep + collectivism + 
                                 longevity + meas_type + rater + design + art_focus,
                        data = datasets_sampled_reduced_best,
                        study = "study_id",
                        vi = "var_adj",
                        whichweights = "random",
                        num.trees = 20000)

plot(check_conv)


# Model with 5000 trees for replication
mf_rep <- MetaForest(r_adj ~ domain + year_merged + complexity + interdep + collectivism + 
                                 longevity + meas_type + rater + design + art_focus,
                        data = datasets_sampled_reduced_best,
                        study = "study_id",
                        vi = "var_adj",
                        whichweights = "random",
                        num.trees = 5000)

# Run recursive preselection, store results in object 'preselect'
preselected <- preselect(mf_rep,
                         replications = 100,
                         algorithm = "recursive")
# Plot the results
plot(preselected)
# Retain only moderators with positive variable importance in more than
# 50% of replications
retain_mods <- preselect_vars(preselected, cutoff = .5)

# Load the caret library
library(caret)
# Set up 10-fold grouped (=clustered) CV
grouped_cv <- trainControl(method = "cv", 
                           index = groupKFold(datasets_sampled_reduced_best$study_id, k = 10))

# Set up a tuning grid for the three tuning parameters of MetaForest
tuning_grid <- expand.grid(whichweights = c("random", "fixed", "unif"),
                       mtry = 2:6,
                       min.node.size = 2:6)

# X should contain only retained moderators, clustering variable, and vi
X <- datasets_sampled_reduced_best[, c("study_id", "var_adj", retain_mods)] %>% rename(vi = var_adj)

# Train the model
mf_cv <- train(y = datasets_sampled_reduced_best$r_adj,
               x = X,
               study = "study_id",
               method = ModelInfo_mf(), 
               trControl = grouped_cv,
               tuneGrid = tuning_grid,
               num.trees = 5000)
# Examine optimal tuning parameters
mf_cv$results[which.min(mf_cv$results$RMSE), ]

# For convenience, extract final model
final <- mf_cv$finalModel
# Extract R^2_{oob} from the final model
r2_oob <- final$forest$r.squared
# Plot convergence
plot(final)

# Plot variable importance
VarImpPlot(final)
# Sort the variable names by importance, so that the
# partial dependence plots will be ranked by importance
ordered_vars <- names(final$forest$variable.importance)[
  order(final$forest$variable.importance, decreasing = TRUE)]
# Plot partial dependence
PartialDependence(final, vars = ordered_vars,
                  rawdata = TRUE, pi = .95, alpha = .01)

PartialDependence(final, vars = ordered_vars,
                  rawdata = FALSE, pi = .95, alpha = .01)

```

# Publication bias

```{r}
datasets_pub <- dataset %>% 
  filter(pub_status == "Published") %>% 
  split(.$domain)

# Exploratory analysis - are *claims* about diversity shaped by publication bias?
datasets_pub_focal <- dataset %>% 
  filter(pub_status == "Published", art_focus == "focal H") %>% 
  split(.$domain)
```


(Rodgers & Pustejovsky, 2021). In line with the simulation results and recommendations by Rodgers and Pustejovsky, we used two methods to test for publication bias. Firstly, 

## Funnel plots

We used an Egger’s regression test to assess the asymmetry of the funnel plot, with Robust Variance Estimation (RVE) taking care of dependence between effect sizes. In order to strike an appropriate balance between statistical power and Type I errors, we followed the common practice highlighted by Siegel and colleagues (2021) and interpreted p-values below .1 as evidence for publication bias.

Given that publication bias tests are assessments of what is reported in the literature, they are based on *unadjusted* correlation coefficients and standard errors.

TK: Consider use of sample size rather than standard error (as standard error depends on the effect size, and has therefore been criticised as a measure of precision here) - or of z-values instead of r? However, that deviates from standard Egger's regression tests, so would need to be exploratory

```{r}

funnel_plots <- list()

for (domain in names(datasets_pub)) {

cur_data <- datasets_pub[[domain]] 

# Load earlier estimate?
confidence_intervals

estimate <- confidence_intervals$beta[confidence_intervals$Coef == paste0("domain", domain)]
se <- confidence_intervals$SE[confidence_intervals$Coef == paste0("domain", domain)]

# Now, compute vectors of the lower-limit and upper limit values for the 95% CI around the metanalytic estimate
se.seq = seq(0,  max(cur_data$se), 0.001)
ll95 = estimate - (1.96*se.seq)
ul95 = estimate + (1.96*se.seq)

# And the various around 0
ll95_0 = 0 - (1.96*se.seq)
ul95_0 = 0 + (1.96*se.seq)
ll90_0 = 0 - (1.65*se.seq)
ul90_0 = 0 + (1.65*se.seq)
ll99_0 = 0 - (2.58*se.seq)
ul99_0 = 0 + (2.58*se.seq)

# Now, smash all of those calculated values into one data frame (called 'dfCI').
dfCI <- data.frame(ll95, ul95, ll95_0, ul95_0, ll99_0, ul99_0, ll90_0, ul90_0, se.seq, estimate)
 
# Add a color column to the dataset based on the specified criteria
cur_data$color <- ifelse(cur_data$art_focus == "focal H", 
                        "darkblue",
                        "lightblue")


   V <- with(cur_data,
            impute_covariance_matrix(vi = se^2,
                                    cluster = articlestudy,
                                    r = rho))

    egger_multi <- rma.mv(yi = r_rep, V = V, random = ~ 1 | articlestudy / effect_id, mods = ~ se, data = cur_data)
    
    
    coefs <-  coef_test(egger_multi, vcov = "CR2")
    
    eggers_label <- glue::glue("Egger's regression test for {domain %>% tolower()} diversity (with RVE): β = {round(coefs$beta[2], 2)}, *t*({round(coefs$df_Satt[2], 1)}) = {round(coefs$t[2], 2)}, *p* {fmt_p(coefs$p[2])}")

fill_colors <- c("n.s." = "white", "*p* < .1" = "#90EE90", "*p* < .05" = "#32CD32", "*p* < .01" = "#006400")

# Make the funnel plot.
funnel_plots[domain] <- list(ggplot(data = cur_data) +
  xlab('r') + ylab('Standard Error') +
    annotate("rect", xmin = -1, xmax = 1, ymin = 0, ymax = max(dfCI$se.seq), fill = "#006400") +
  geom_ribbon(data = dfCI, aes(xmin = ll99_0, xmax = ul99_0, y = se.seq, fill = "*p* < .05")) +
  geom_ribbon(data = dfCI, aes(xmin = ll95_0, xmax = ul95_0, y = se.seq, fill = "*p* < .1" )) +
geom_ribbon(data = dfCI, aes(xmin = ll90_0, xmax = ul90_0, y = se.seq, fill = "n.s.")) +
  # Dummy layer for the annotated color
  geom_ribbon(aes(y = 0, xmin = Inf, xmax = Inf, fill = "*p* < .01")) +

    scale_fill_manual("Significance", values = fill_colors) + 
    geom_point(aes(y = se, x = r_rep, color = color), alpha = .6) +
  scale_color_identity() +
  geom_line(aes(y = se.seq, x = ll95), linetype = 'dotted', linewidth = .5, data = dfCI) +
  geom_line(aes(y = se.seq, x = ul95), linetype = 'dotted', linewidth = .5, data = dfCI) +
  geom_line(aes(x = estimate, y = se_range),
            data = tibble(estimate = estimate, se_range = c(0, max(dfCI$se.seq))),
            linetype = 'dotted', color = 'darkblue', linewidth = .5) +
  scale_x_continuous() +
  coord_cartesian(xlim = c(-1, 1), ylim = c(max(dfCI$se.seq), 0), expand = FALSE) +
  #scale_y_continuous(trans = "reverse", limits = c(0, -max(dfCI$se.seq))) + 
  labs(title = paste0("Funnel plot for ", domain %>% str_remove("domain") %>% tolower(), " diversity"),
       subtitle = "95% and 99% confidence intervals",
       caption = eggers_label, x = "*r*", y = "Standard error") +
    theme(plot.caption = ggtext::element_markdown(),
          axis.title.x = ggtext::element_markdown(),
          legend.text = ggtext::element_markdown(),
          panel.grid = element_blank()))
}

funnel_plots


```

No evidence that effect size is associated with standard error - so that positive results appear to be no more likely to be published than 

## 3 PSM

Secondly, we planned to use the 3-parameter selection model (3PSM) to directly estimate whether non-significant results have a lower chance of being published than significant findings. However, this model assumes selection for direction *and* significance, which would be inappropriate for the current data where hypotheses point in varying directions. Consequently, we extended the model to estimate selection for both positive and negative significant results by including two cut-points.

Selection models (of either type) cannot presently be extended to account for dependent effect sizes but sampling one effect size per sample results in a test that combines comparatively high power with a predictable Type I error rate. Therefore, we bootstrapped 3PSM with effect size sampling, and report the median results and distribution of 5,000 bootstrap resamples. Given that an alpha level of .05 is associated with a Type I error rate of up to 10%, we relied on this threshold (Rodgers & Pustejovsky, 2021). 

Roders & Pustejovsky (2023?) show that sampling leads to much lower Type I error rates than ignoring dependency or aggregating effects for 3PSM

Most recent (preliminary) results by Pustejovsky (2023 - https://www.jepusto.com/cluster-bootstrap-selection-model/) suggest that the cluster bootstraps (i.e. sampling at the study rather than the effect size level) may be the most accurate method for estimating selection models in the presence of dependent effects, so that we present this approach as supplementary.


```{r}
# Define a function for the selection model
apply_selmodel <- function(data, boot_method = c("within", "cluster")) {
  if (boot_method == "within") {
     data <- data %>% group_by(articlestudy) %>% sample_n(1) %>% ungroup() %>%   # Sample one effect size per study
       sample_n(nrow(.), replace = TRUE) # Create bootstrap sample
  } else if (boot_method == "cluster") {
    data <- data %>% nest(.by = articlestudy, .key = "data") %>% sample_n(nrow(.), replace = TRUE) %>% unnest(data)
  } else {
    stop("Invalid boot_method")
  }
    run_selmodel <- function (data) {
    # Fit the model
    m.rma <- rma(yi = r_rep,        
                sei = se,
                data = data,
                slab = articlestudy,
                method = "REML",
                test = "knha")
    
    selmodel(m.rma,
            type = "stepfun",
            skiphes = TRUE, skiphet = TRUE, # turn off SE and het-test calculations to accelerate,
            steps = c(0.025, .975)) %>%  # Selection for both positive and negative sig values
        summary()
    
    }
    
    run_selmodel <- purrr::possibly(run_selmodel, otherwise = NA)
    
    psm_mod <- run_selmodel(data)
    
    if (is.na(psm_mod[1])) {
      return(tibble()) 
    } else {

    # Create dataframe with the required elements
    tibble(
        lrt = psm_mod$LRT, 
        p_table = list(psm_mod$ptable %>% mutate(prob = psm_mod$delta))
    )
    }
}

# Bootstrap sampling and applying 3PSM
set.seed(123) # for reproducibility
n_bootstraps <- 100

handlers("cli")

plan(multisession, workers = 6)
tic()
with_progress({
   p <- progressor(steps = n_bootstraps * 3, label = "Bootstrapping")
sel_models_within <- map_dfr(datasets_pub, \(domain_data) {
  future_map_dfr(seq_len(n_bootstraps) %>% set_names(), \(i) {
    p()
    apply_selmodel(domain_data, boot_method = "within")
  }, .id = "run", .options = furrr_options(seed = TRUE))}, .id = "domain")  
 })
toc()


tic()
with_progress({
   p <- progressor(steps = n_bootstraps * 3, label = "Bootstrapping")
   sel_models_cluster <- map_dfr(datasets_pub, \(domain_data) {
   future_map_dfr(seq_len(n_bootstraps) %>% set_names(), \(i) {
     p()
     apply_selmodel(domain_data, boot_method = "cluster")
   }, .id = "run", .options = furrr_options(seed = TRUE))}, .id = "domain")  
 })
toc()

tic()
with_progress({
   p <- progressor(steps = n_bootstraps * 3, label = "Bootstrapping")
sel_models_within_focal <- map_dfr(datasets_pub_focal, \(domain_data) {
  future_map_dfr(seq_len(n_bootstraps) %>% set_names(), \(i) {
    p()
    apply_selmodel(domain_data, boot_method = "within")
  }, .id = "run", .options = furrr_options(seed = TRUE))}, .id = "domain")  
 })
toc()


tic()
with_progress({
   p <- progressor(steps = n_bootstraps * 3, label = "Bootstrapping")
   sel_models_cluster_focal <- map_dfr(datasets_pub_focal, \(domain_data) {
   future_map_dfr(seq_len(n_bootstraps) %>% set_names(), \(i) {
     p()
     apply_selmodel(domain_data, boot_method = "cluster")
   }, .id = "run", .options = furrr_options(seed = TRUE))}, .id = "domain")  
 })
toc()



# Significance tests
sel_models_within %>% 
  group_by(domain) %>% 
  summarise(ci_lower = quantile(lrt, .025),
            ci_upper = quantile(lrt, .975),
            lrt = median(lrt), 
            .groups = "drop") %>% 
  mutate(p = pchisq(lrt, df = 2, lower.tail = FALSE))





sel_models_cluster %>% 
  group_by(domain) %>% 
  summarise(ci_lower = quantile(lrt, .025),
            ci_upper = quantile(lrt, .975),
            lrt = median(lrt), 
            .groups = "drop") %>% 
  mutate(p = pchisq(lrt, df = 2, lower.tail = FALSE))

# Significance tests

# Calculate median estimates
# TK - do this once retun values are clear
median_estimate <- median(all_estimates)

# Visualization
```


```{r}
# Visualisation derived from code by Cédric Scherer (https://gist.github.com/z3tt/8b2a06d05e8fae308abbf027ce357f01) - thanks!
library(ggtext)
library(colorspace)
library(ragg)

pal <- c("#A034F0", "#159090")

dat <- sel_models_within %>%
  mutate(method = "sampling") %>%
  bind_rows(sel_models_cluster %>% mutate(method = "cluster")) %>%
  mutate(hypotheses = "all") %>%
  bind_rows(sel_models_cluster_focal %>% mutate(method = "cluster") %>% 
              bind_rows(sel_models_within_focal %>% mutate(method = "sampling")) %>% mutate(hypotheses = "focal")) %>%
  mutate(p_table = map(p_table, \(x) x %>% rownames_to_column("p_cat"))) %>%
  unnest(p_table) %>%
  mutate(p_cat = case_when(
    p_cat == "0     < p <= 0.025" ~ "*p* < .05 (neg)",
    p_cat == "0.025 < p <= 0.975" ~ "n.s.",
    p_cat == "0.975 < p <= 1" ~ "*p* < .05 (pos)"
  ) %>% factor()) %>%
  filter(p_cat != "*p* < .05 (neg)")

dat %>% 
  group_by(hypotheses, domain, p_cat) %>% 
  summarise(prob_median = median(prob), 
            prob_mean = mean(prob),
            prob_se = sd(prob),
            ci_lower = quantile(prob, .025),
            ci_upper = quantile(prob, .975),
            .groups = "drop") %>% 
  gt() %>% fmt_number(decimals = 3)

dat %>%
  ggplot(aes(x = fct_rev(p_cat), y = prob, color = method, fill = method), alpha = .5) +
  facet_grid(~domain + hypotheses) +
  ggdist::stat_halfeye(
    aes(fill = after_scale(lighten(color, .5))),
    adjust = .5,
    width = .75,
    .width = 0,
    justification = -.4,
    point_color = NA,
    alpha = .7
  ) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "grey40") +
  stat_summary(
    geom = "text",
    fun = "median",
    aes(
      label = round(after_stat(y), 2),
      color = method,
      y = stage(prob, after_stat = 3.5 + as.numeric(color == "sampling") * .3),
      color = after_scale(darken(color, .1, space = "HLS"))
    ),
    # family = "Roboto Mono",
    fontface = "bold",
    size = 3.3,
    # size = 4,
    #vjust = -4,
    position = position_dodge(width = .5)
  ) +
  stat_summary(
    geom = "point",
    fun = "median",
    aes(
      label = round(after_stat(y), 2),
      color = method,
      color = after_scale(darken(color, .1, space = "HLS"))
    ),
    # family = "Roboto Mono",
    shape = "—",
    size = 4,
    position = position_dodge(width = .5)
  ) +
  stat_summary(
    fun.data = function(y) {
      data.frame(
        y = mean(y),
        ymin = quantile(y, .025),
        ymax = quantile(y, .975)
      )
    },
    geom = "errorbar",
    width = 0.2,
    position = position_dodge(width = .5)
  ) +
  scale_color_manual(values = pal, guide = "none") +
  scale_fill_manual(values = pal) +
  labs(
    x = "",
    y = "**Publication odds**<br>(vs. *p* < .05 for negative results)",
    title = "Publication bias as per selection models",
    caption = "Results of 5,000 bootstrap resamples. Labeled midpoints represent medians, bars 95% CIs.",
    fill = "Bootstrap method",
  ) +
  scale_y_continuous(limits = c(0, 5)) +
  theme(axis.title.y = ggtext::element_markdown(),
        axis.text.x = ggtext::element_markdown(),
          legend.text = ggtext::element_markdown(),
          panel.grid = element_blank())
```



# Stat checks

## GRIM and GRIMMER

```{r}

# Means are imported as character so that trailing 0s can be accounted for - therefore,
# we use this wrapper function to convert to numeric and account for trailing 0s

GRIM_test_wrapper <- function(mean_char, n_obs, n_items = 1, return_values = FALSE) {
  # Convert mean from character to numeric
  mean_num <- as.numeric(mean_char)
  
  # Determine the precision of the mean based on the input string
  decimal_position <- regexpr("\\.", mean_char)
  if (decimal_position == -1) {
    m_prec <- 0
  } else {
    m_prec <- nchar(substring(mean_char, decimal_position + 1))
  }
  
  # Call the original GRIM_test function with calculated precision
  GRIM_test(mean = mean_num, n_obs = n_obs, m_prec = m_prec, n_items = n_items, return_values = return_values)
}

GRIMMER_test_wrapper <- function(mean_char, sd_char, n_obs, n_items = 1) {
  # Convert mean from character to numeric
  mean_num <- as.numeric(mean_char)
  sd_num <- as.numeric(sd_char)
  
  # Determine the precision of the mean based on the input string
  decimal_position <- regexpr("\\.", mean_char)
  if (decimal_position == -1) {
    m_prec <- 0
  } else {
    m_prec <- nchar(substring(mean_char, decimal_position + 1))
  }
  decimal_position <- regexpr("\\.", sd_char)
  if (decimal_position == -1) {
    sd_prec <- 0
  } else {
    sd_prec <- nchar(substring(sd_char, decimal_position + 1))
  }
  
  # Call the original GRIM_test function with calculated precision
  GRIMMER_test(mean = mean_num, sd = sd_num, n_obs = n_obs, m_prec = m_prec, sd_prec = sd_prec, n_items = n_items)
}

GRIM_div_concerns <- dataset %>% filter(!is.na(m_div)) %>% 
  mutate(N = ifelse(!is.na(notes_div) & str_detect(notes_div, "N = "), str_extract(notes_div, "\\d+") %>% as.numeric(), n_teams_coded)) %>% 
  rowwise() %>% 
  mutate(GRIM = GRIM_test_wrapper(m_div, N, n_items = items_div, return_values = FALSE),
         GRIMMER = GRIMMER_test_wrapper(m_div, sd_div, N, items_div)) %>% 
  filter(!GRIMMER)

GRIM_perf_concerns <- dataset %>% filter(!is.na(m_perf)) %>% 
  mutate(N = ifelse(!is.na(notes_perf) & str_detect(notes_perf, "N = "), str_extract(notes_perf, "\\d+") %>% as.numeric(), n_teams_coded)) %>% 
  rowwise() %>% 
  mutate(GRIM = GRIM_test_wrapper(m_perf, N, n_items = items_perf, return_values = FALSE),
         GRIMMER = GRIMMER_test_wrapper(m_perf, sd_perf, N, items_perf)) %>% 
  filter(!GRIMMER)

GRIM_div_concerns %>% transmute(id, diversity = paste(domain, sub_dom, div_specific), m_div, N, items_div, GRIM)
GRIM_perf_concerns %>% transmute(id, performance = paste(name_perf, rater), m_perf, N, items_perf, GRIM)

GRIM_flagged_reviewed <- c("10.1348/096317905x72128", "10.1111/joop.12303")

```

Investigation of individual concerns:
- 10.1108/jmp-01-2012-0020: Reports impossible mean (2.47) for functional diversity *if* this was based on the full sample (N = 48). It could be limited to the sample where manager ratings are available (N = 36), but that is not indicated in the reporting.
- 10.1111/joop.12303: Performance ratings inconsistent with our interpretation of the measure (as single-item). Likely three items, but that is not clearly indicated, and no reliability is reported.

## Statcheck

```{r}

extract_pdfminer <- function(path) {
  # Import Python module
  pdfminer <- import("pdfminer.high_level")
  
  # Call Python function to extract text
  text <- pdfminer$extract_text(path)
  
  # Return the extracted text
  return(text)
}

extract_pdfminer("../SM1 - Search and deduplication/full_text/included//10.1177--1368430212437798.pdf") %>% str_squish() %>% clipr::write_clip()

# Retrieve relevant FTs
full_texts <- list.files("../SM1 - Search and deduplication/full_text/included/", full.names = TRUE, recursive = TRUE, pattern = ".pdf")

pdfs$path <- full_texts[map_int(pdfs$file_name, \(f) str_detect(full_texts, fixed(f)) %>% which() %>% c(NA) %>% .[1])]

# Two PDFs missing - here authors of conference papers provided brief excerpts only
# pdfs %>% filter(is.na(path)) %>% left_join(dataset %>% select(id, file), by = "id", multiple = "first") 

run_statcheck <- function(file_path, pdf_engine = c("default", "pdftools", "all"), ocr_failures = TRUE, suppress_ocrmypdf = FALSE) {
  if(!file.exists(file_path)) {
    warning("File does not exist: ", file_path)
    return(NA)
  }
  
  statcheck_result <- list()
  
  if (pdf_engine[1] %in% c("pdftools", "all")) {
  pdf_text <- pdftools::pdf_text(file_path) %>% 
              paste(collapse = "") %>% 
              str_squish() %>%
    str_remove_all('-(?=\\s*\\d)') # remove space between - and a number
  
  statcheck_result["pdftools"] <- list(possibly(statcheck::statcheck, otherwise = NULL)(pdf_text, pZeroError = FALSE, OneTailedTxt = TRUE, messages = FALSE))

  } 
  
  if (pdf_engine[1] %in% c("default", "all")) {
    statcheck_result["default"] <- list(possibly(statcheck::checkPDF, otherwise = NULL)(file_path, pZeroError = FALSE, OneTailedTxt = TRUE, messages = FALSE))
  }
  
  if ((is.null(statcheck_result) && ocr_failures == TRUE)|| pdf_engine[1] %in% c("all")) {
    #message("Attempting OCR ...")
    if (suppress_ocrmypdf == TRUE) {
if (.Platform$OS.type == "windows") {
  redirect <- " >NUL 2>&1"
} else {
  redirect <- " >/dev/null 2>&1"
}
    } else {
      redirect <- ""
    }
    
    temp <- tempfile(fileext = ".pdf")
    system(sprintf('ocrmypdf --force-ocr --quiet %s %s %s', 
                   shQuote(file_path),
                   shQuote(temp),
                   redirect))
    statcheck_result["ocr"] <- list(possibly(statcheck::checkPDF, otherwise = NULL)(temp, pZeroError = FALSE, OneTailedTxt = TRUE, messages = FALSE))

    if(!is.null(statcheck_result[["ocr"]])) {
      #message("OCR improved ", basename(file_path))
    } else {
      #message("OCR did not change ", basename(file_path))
    }
  }
  
  if (is.null(statcheck_result)) {
    return(NA)
  } else {
    return(statcheck_result)
  }
}



# Run as background job
# Does not return data, but saves it to disk
job::job({
plan(multisession, workers = 4)
with_progress({
  p <- progressor(steps = nrow(pdfs))
  
  # Create a directory for the intermittent results if it doesn't exist
  results_dir <- "data/statcheck_results"
  if (!dir.exists(results_dir)) dir.create(results_dir)
  
  statcheck_results <- future_walk2(pdfs$path, pdfs$file_name, \(f, id) {
    result_file <- file.path(results_dir, paste0(id, ".rds"))
    # Check if the result for this file already exists to skip processing
    if (!file.exists(result_file)) {
      result <- possibly(run_statcheck, otherwise = NULL)(f, pdf_engine = "all", suppress_ocrmypdf = FALSE)
      if (is.null(result)) {
        browser()
      }
      # Save the result to disk
      saveRDS(result, result_file)
      t <- try(readRDS(result_file))
if(class(t) == "try-error") {
    browser()
}
    }
    
    p() # Update progress
    #result
  })
})},
title = "Run statcheck with multiple extraction options"
)

# Read the results from disk
results_dir <- "data/statcheck_results"
statcheck_results <- map2(pdfs$id, pdfs$file_name, \(id, f) {
    dat <- possibly(read_rds, otherwise = tibble(status = "RDS failed", id = id))(file.path(results_dir, paste0(f, ".rds"))) 
    if (!is_tibble(dat)) {
     
  dat <- dat %>% 
    map_dfr(\(x) if(is.null(x)) {
      tibble(status = "failed")
    }  else if (length(x) == 1 && is.na(x)) {
              tibble(status = "no results found")
      } else if (!(is.data.frame(x))) {
        tibble(status = "invalid file type") # should never get here
      } else {
        x %>% mutate(status = "succeeded")
        }, .id = "method") %>% 
    mutate(id = id)
}
    dat}
    ) %>% bind_rows() 

statcheck_results %>% 
  group_by(method, status, id) %>% 
  summarise(n_res = ifelse(status[1] == "succeeded", n(), 0), .groups = "drop_last") %>%
  filter(status == "RDS failed") %>% pull(id)

statcheck_results %>% 
  group_by(method, status, id) %>% 
  summarise(n_res = ifelse(status[1] == "succeeded", n(), 0), .groups = "drop_last") %>%
  summarise(n = n(), mean_res = mean(n_res), median_res = median(n_res), .groups = "drop") %>% 
  filter(status == "succeeded") %>% 
  mutate(success_rate = n / nrow(pdfs))
 
statcheck_merged <- statcheck_results %>%
  filter(status == "succeeded") %>% 
  select(-source, -status, -apa_factor) %>% 
  distinct() %>% 
  mutate(value = TRUE) %>%
  pivot_wider(names_from = method, values_from = value, values_fill = FALSE) %>% 
  group_by(across(-c(pdftools:default))) %>% 
  summarise(across(pdftools:default, any), .groups = 'drop')

# Before reporting on parsing methods, would need to improve merging (e.g., with spaces or different dashes)
statcheck_merged %>% count(across(pdftools:default)) %>% 
  mutate(hits = pdftools + ocr + default) %>% arrange(-hits) %>% 
  select(hits, everything())

statcheck_merged %>% filter(decision_error)
statcheck_merged %>% filter(error, !decision_error) %>% mutate(computed_p = round(computed_p, 3))

statcheck_flagged_reviewed <- c("10.1037/1089-2699.5.2.111", "10.1080/00140139.2013.875597", "10.1109/tem.2011.2166078", "10.1177/1368430212437798", "10.1287/orsc.2013.0878", "21073", "38862", "10.1002/hrm.21658", "10.1002/job.1777", "10.1016/j.jbusres.2018.11.029", "10.1016/j.obhdp.2013.04.003", "10.1037/1089-2699.12.4.307", "10.1037/a0025583", "10.1109/ieem.2013.6962412", "10.1177/1046496418796281", "10.1348/096317905x72128", "10.2139/ssrn.2929009", "31242", "34459", "41373", "9990")


```

Reviewed decision errors:
- 10.1037/1089-2699.5.2.111: x(3, N = 321) = 63.44, p >.1 *inconsistent*
- 10.1080/00140139.2013.875597: parse errors, Chi2 should be t, BUT flagged F tests inconsistent with ps 
- 10.1109/tem.2011.2166078: flagged chi-sq plus the third reported all appear inconsistent
- OK: 10.1177/00187208211048301: one-tailed test, reported but too far for statcheck to catch automatically
- 10.1177/1368430212437798: Sobel's test using z, so possibly some flexibility - but can't see how that p-value could be achieved (and 5 other reporting errors without decision errors)
- 10.1287/orsc.2013.0878: maybe have used one-tailed tests for Sobel's test without reporting that - inconsistent as it stands (and again for test interpreted as showing marginal significance)
- OK: 10.1287/orsc.2021.1448: parse error, Chi2 should be t, then consistent
- OK: 10.1287/orsc.2021.1560: parse error, Chi2 should be t, then consistent
- 21073: flagged F-test inconsistent - as is a previous one claimed to be significant at 10% level
- 38862: inconsistent F-tests
- OK: bw_15842: parse error, Chi2 should be t, then consistent


Reviewed other errors:
- 10.1002/hrm.21658	- inconsistent, conservative
- 10.1002/job.1777	- inconsistent, conservative
- 10.1016/j.jbusres.2018.11.029	- inconsistent, conservative
- 10.1016/j.obhdp.2013.04.003	- inconsistent
- 10.1037/1089-2699.12.4.307	- inconsistent
- 10.1037/a0025583	- inconsistent
- OK 10.1097/HMR.0000000000000369 - p reported as < .00, which is impossible, but p is indeed tiny, so accepted
- 10.1109/ieem.2013.6962412 - appears to have used one-tailed p-values without reporting that
- OK 10.1109/ieem.2014.7058744	- parsing error because , instead of . was used for decimals
- 10.1177/1046496418796281: inconsistent, conservative
- 10.1348/096317905x72128	- inconsistent, very minor (and again many instances of p < .00 that would be ok on their own)
- 10.2139/ssrn.2929009	- inconsistent, wrong band (i.e. not < .01)
- 31242 - inconsistent, conservative
- OK 33800, issue is only < .00 (plus one , vs . parsing error)
- 34459 - inconsistent, though only = .001 instead of < .001
- 41373 - inconsistent, though only < .004 instead of = .004
- 9990 - inconsistent, minor

